[{"content":"BeautifulSoup 简介 BeautifulSoup 是一个可以把 HTML 或 XML 文档“变成结构化树”，方便你用Python 来查找、遍历、提取内容的工具。\n解析器 具体使用代码展示如下：\n1 2 3 4 from bs4 import BeautifulSoup soup = BeautifulSoup(\u0026#39;\u0026lt;p\u0026gt;Hello world\u0026lt;/p\u0026gt;\u0026#39;, \u0026#39;lxml\u0026#39;) print(soup.p) 基本用法 当传入不标准的HTML字符串，BeautifulSoup可以自动更正格式。\nsoup.prettify(): 用于将 HTML 或 XML 文档格式化输出为带缩进的字符串,即会把原始网页内容重新排版，输出一个带有层级缩进的漂亮 HTML 字符串，例：\n1 2 3 4 5 6 7 from bs4 import BeautifulSoup html = \u0026#39;\u0026lt;html\u0026gt;\u0026lt;head\u0026gt;\u0026lt;title\u0026gt;Test\u0026lt;/title\u0026gt;\u0026lt;/head\u0026gt;\u0026lt;body\u0026gt;\u0026lt;h1\u0026gt;Hi\u0026lt;/h1\u0026gt;\u0026lt;p\u0026gt;Hello\u0026lt;/p\u0026gt;\u0026lt;/body\u0026gt;\u0026lt;/html\u0026gt;\u0026#39; soup = BeautifulSoup(html, \u0026#39;html.parser\u0026#39;) pretty_html = soup.prettify() print(pretty_html) 输出：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt; Test \u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt; Hi \u0026lt;/h1\u0026gt; \u0026lt;p\u0026gt; Hello \u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; .title：获取title标签\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 html_doc=\u0026#34;\u0026#34;\u0026#34; \u0026lt;html\u0026gt;\u0026lt;head\u0026gt;\u0026lt;title\u0026gt;The Dormouse\u0026#39;s story\u0026lt;/title\u0026gt;\u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p class=\u0026#34;title\u0026#34;\u0026gt;\u0026lt;b\u0026gt;The Dormouse\u0026#39;s story\u0026lt;/b\u0026gt;\u0026lt;/p\u0026gt; \u0026lt;p class=\u0026#34;story\u0026#34;\u0026gt;Once upon a time there were three little sisters; and their names were \u0026lt;a href=\u0026#34;http://example.com/elsie\u0026#34; class=\u0026#34;sister\u0026#34; id=\u0026#34;link1\u0026#34;\u0026gt;Elsie\u0026lt;/a\u0026gt;, \u0026lt;a href=\u0026#34;http://example.com/lacie\u0026#34; class=\u0026#34;sister\u0026#34; id=\u0026#34;link2\u0026#34;\u0026gt;Lacie\u0026lt;/a\u0026gt; and \u0026lt;a href=\u0026#34;http://example.com/tillie\u0026#34; class=\u0026#34;sister\u0026#34; id=\u0026#34;link3\u0026#34;\u0026gt;Tillie\u0026lt;/a\u0026gt;; and they lived at the bottom of a well.\u0026lt;/p\u0026gt; \u0026lt;p class=\u0026#34;story\u0026#34;\u0026gt;...\u0026lt;/p\u0026gt; \u0026#34;\u0026#34;\u0026#34;\u0026#34; # 创建beautifulsoup对象 解析器为lxml soup = BeautifulSoup(html_doc, \u0026#39;lxml\u0026#39;) print(soup.title) #output-\u0026gt;\u0026lt;title\u0026gt;The Dormouse\u0026#39;s story\u0026lt;/title\u0026gt; .name：返回的是当前节点的“标签名称”\n1 2 3 4 5 soup = BeautifulSoup(html_doc, \u0026#39;lxml\u0026#39;) print(soup.title.name) print(soup.name) #output-\u0026gt;title #[document] .string/.text：获取标签中的文字内容\n1 2 3 4 5 soup = BeautifulSoup(html_doc, \u0026#39;lxml\u0026#39;) print(soup.title.string) print(soup.title.text) #output-\u0026gt;The Dormouse\u0026#39;s story #The Dormouse\u0026#39;s story .p：访问HTML中第一个标签\n1 2 3 soup = BeautifulSoup(html_doc, \u0026#39;lxml\u0026#39;) print(soup.p) #output-\u0026gt;\u0026lt;p class=\u0026#34;title\u0026#34;\u0026gt;\u0026lt;b\u0026gt;The Dormouse\u0026#39;s story\u0026lt;/b\u0026gt;\u0026lt;/p\u0026gt; .find_all()：查找文档中所有符合条件的标签元素，返回一个列表\n用法 示例 说明 查找所有某种标签 soup.find_all(\u0026lsquo;p\u0026rsquo;) 找出所有 \u0026lt;p\u0026gt; 根据 class 属性 soup.find_all(\u0026lsquo;p\u0026rsquo;, class_=\u0026lsquo;story\u0026rsquo;) class 要用 class_ 表示 查找多个标签 soup.find_all([\u0026lsquo;p\u0026rsquo;, \u0026lsquo;a\u0026rsquo;]) 所有 \u0026lt;p\u0026gt; 和 \u0026lt;a\u0026gt; 标签 查找包含特定字符串的标签 soup.find_all(string=\u0026lsquo;Hello\u0026rsquo;) 内容匹配为 \u0026lsquo;Hello\u0026rsquo; 的标签 使用属性字典 soup.find_all(\u0026lsquo;a\u0026rsquo;, {\u0026lsquo;id\u0026rsquo;: \u0026rsquo;link1\u0026rsquo;}) 属性筛选 限制返回数量 soup.find_all(\u0026lsquo;p\u0026rsquo;, limit=2) 最多返回 2 个 \u0026lt;p\u0026gt; 标签 1 2 3 4 \u0026#39;\u0026#39;\u0026#39; 基本语法： soup.find_all(name=None, attrs={}, recursive=True, string=None, limit=None, **kwargs) \u0026#39;\u0026#39;\u0026#39; .find()：获取第一次匹配条件的元素\n1 2 3 soup = BeautifulSoup(html_doc, \u0026#39;lxml\u0026#39;) print(soup.find(id=\u0026#34;link1\u0026#34;)) #output-\u0026gt;\u0026lt;a class=\u0026#34;sister\u0026#34; href=\u0026#34;https://example.com/elsie\u0026#34; id=\u0026#34;link1\u0026#34;\u0026gt;Elsie\u0026lt;/a\u0026gt; .parent：获取父级标签\n1 2 3 4 5 soup = BeautifulSoup(html_doc, \u0026#39;lxml\u0026#39;) print(soup.title) print(soup.title.parnt) #output-\u0026gt;\u0026lt;title\u0026gt;The Dormouse\u0026#39;s story\u0026lt;/title\u0026gt; #\u0026lt;head\u0026gt;\u0026lt;title\u0026gt;The Dormouse\u0026#39;s story\u0026lt;/title\u0026gt;\u0026lt;/head\u0026gt; .p[\u0026lsquo;class\u0026rsquo;]：获取某个 \u0026lt;p\u0026gt; 标签的 class 属性的值\n1 2 3 soup = BeautifulSoup(html_doc, \u0026#39;lxml\u0026#39;) print(soup.p[\u0026#34;class\u0026#34;]) #output-\u0026gt;[\u0026#39;title\u0026#39;] .get_text()：获取文档中所有文字内容\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 soup = BeautifulSoup(html_doc, \u0026#39;lxml\u0026#39;) print(soup.get_text()) \u0026#39;\u0026#39;\u0026#39; output-\u0026gt; The Dormouse\u0026#39;s story The Dormouse\u0026#39;s story Once upon a time there were three little sisters; and their names were Elsie, Lacie and Tillie; and they lived at the bottom of a well. ... \u0026#39;\u0026#39;\u0026#39; .get()：用于安全地获取某个属性的值\n1 2 3 4 5 6 7 8 9 \u0026#39;\u0026#39;\u0026#39; tag.get(\u0026#39;属性名\u0026#39;) 相当于 tag[\u0026#39;属性名\u0026#39;]，但如果属性不存在，不会报错，而是返回 None \u0026#39;\u0026#39;\u0026#39; a_tags = soup.find_all(\u0026#39;a\u0026#39;) for a_tag in a_tags: print(a_tag.get(\u0026#34;href\u0026#34;)) #output-\u0026gt;https://example.com/elsie #https://example.com/lacie #https://example.com/tillie BeautifulSoup的对象种类 主要有四种主要类型：Tag，NavigableString，BeautifulSoup，Comment。\nTag：该对象与HTML或XML原生文档中的标签相同，该对象可以包含其他标签，文本内容和属性。\n1 2 3 4 soup = BeautifulSoup(html_doc, \u0026#39;lxml\u0026#39;) tag = soup.title print(type(tag)) #output-\u0026gt;\u0026lt;class \u0026#39;bs4.element.Tag\u0026#39;\u0026gt; NavigableString：标签中的文本内容，是一个不可变字符串，可以由Tag对象的.string获取\n1 2 3 4 soup = BeautifulSoup(html_doc, \u0026#39;lxml\u0026#39;) tag = soup.title print(type(tag.string)) #output-\u0026gt; \u0026lt;class \u0026#39;bs4.element.NavigableString\u0026#39;\u0026gt; BeautifulSoup：整个文档的根对象，即整个文档的根内容，可以被视为一个特殊的Tag对象，但没有名称和属性，其提供对整个文档的遍历，搜索和修改\n1 2 3 soup = BeautifulSoup(html_doc, \u0026#39;lxml\u0026#39;) print(type(soup)) #output-\u0026gt; \u0026lt;class \u0026#39;bs4.BeautifulSoup\u0026#39;\u0026gt; Comment：对象是一个特殊类型的NavigableString对象,表示HTML和XML中的注释部分\n1 2 3 4 # \u0026lt;b\u0026gt;\u0026lt;!--This is a comment--\u0026gt;\u0026lt;/b\u0026gt; soup = BeautifulSoup(html_doc, \u0026#39;lxml\u0026#39;) print(type(soup.b.string)) #output-\u0026gt; \u0026lt;class \u0026#39;bs4.element.NavigableString\u0026#39;\u0026gt; BeautifulSoup遍历文档树 BeautifulSoup提供很多方法来遍历解析后的文档树\n导航父节点：.parent和.parents。其中.parent可以获取当前节点的上一级父节点，.parents可以遍历获取当前节点的所有父辈节点\n1 2 3 4 soup = BeautifulSoup(html_doc, \u0026#39;lxml\u0026#39;) title_tag = soup.title print(title_tag.parent) #\u0026lt;head\u0026gt;\u0026lt;title\u0026gt;The Dormouse\u0026#39;s story\u0026lt;/title\u0026gt;\u0026lt;/head\u0026gt; 1 2 3 4 5 6 7 8 9 10 soup = BeautifulSoup(html_doc, \u0026#39;lxml\u0026#39;) body_tag = soup.body for parent in body_tag.parents: print(parent) #\u0026lt;html\u0026gt;\u0026lt;head\u0026gt;\u0026lt;title\u0026gt;The Dormouse\u0026#39;s story\u0026lt;/title\u0026gt;\u0026lt;/head\u0026gt; #\u0026lt;body\u0026gt; #\u0026lt;p class=\u0026#34;title\u0026#34;\u0026gt;\u0026lt;b\u0026gt;The Dormouse\u0026#39;s story\u0026lt;/b\u0026gt;\u0026lt;/p\u0026gt; #\u0026lt;p class=\u0026#34;story\u0026#34;\u0026gt;Once upon a time there were three little sisters; and their names were #\u0026lt;a class=\u0026#34;sister\u0026#34; href=\u0026#34;https://example.com/elsie\u0026#34; id=\u0026#34;link1\u0026#34;\u0026gt;Elsie\u0026lt;/a\u0026gt;, #.... 导航子节点：.contents可以获取当前节点的所有子节点；.children可以遍历当前节点的所有子节点，返回一个list。字符串没有这两个属性。这两个仅包含tag直接子结点，字符串”The Dormouse\u0026rsquo;s story”是\u0026lt;head\u0026gt;标签的子孙结点\n1 2 3 4 soup = BeautifulSoup(html_doc, \u0026#39;lxml\u0026#39;) head_contents = soup.head.contents print(head_contents) #output-\u0026gt; [\u0026lt;title\u0026gt;The Dormouse\u0026#39;s story\u0026lt;/title\u0026gt;] 1 2 3 4 5 6 7 8 9 10 soup = BeautifulSoup(html_doc, \u0026#39;lxml\u0026#39;) body_children = soup.body.children for child in body_children: print(child) #output-\u0026gt;\u0026lt;p class=\u0026#34;title\u0026#34;\u0026gt;\u0026lt;b\u0026gt;The Dormouse\u0026#39;s story\u0026lt;/b\u0026gt;\u0026lt;/p\u0026gt; #\u0026lt;a class=\u0026#34;sister\u0026#34; href=\u0026#34;https://example.com/elsie\u0026#34; id=\u0026#34;link1\u0026#34;\u0026gt;Elsie\u0026lt;/a\u0026gt;, #\u0026lt;a class=\u0026#34;sister\u0026#34; href=\u0026#34;https://example.com/tillie\u0026#34; id=\u0026#34;link3\u0026#34;\u0026gt;Tillie\u0026lt;/a\u0026gt;; #and they lived at the bottom of a well.\u0026lt;/p\u0026gt; #..... .descendants：可以遍历当前节点的所有后代节点（层遍历）\n1 2 3 soup = BeautifulSoup(html_doc, \u0026#39;lxml\u0026#39;) for descendant in soup.descendants: print(descendant) 节点内容：.string，.strings，.stripped_strings。.string如果如果tag只有一个NavigableString类型子节点,那么这个tag可以使用.string得到其子节点。但若tag中包含了多个子节点,tag就无法确定string方法应该调用哪一个字节的内容,则会输出None\n1 2 3 4 5 soup = BeautifulSoup(html_doc, \u0026#39;lxml\u0026#39;) print(soup.head.string) #The Dormouse\u0026#39;s story print(soup.title.string) #The Dormouse\u0026#39;s story 1 2 3 soup = BeautifulSoup(html_doc, \u0026#39;lxml\u0026#39;) print(soup.body.string) #None .strings可以遍历获取标签中的所有文本内容，.stripped_strings可以去除多余的空白字符\n1 2 3 4 5 6 7 soup = BeautifulSoup(html_doc, \u0026#39;lxml\u0026#39;) for string in soup.strings: print(string) #The Dormouse\u0026#39;s story ...... #The Dormouse\u0026#39;s story BeautifulSoup搜索文档树 find_all()：搜索当前tag的所有tag子节点\n1 2 3 4 5 6 7 8 9 10 11 soup = BeautifulSoup(html_doc, \u0026#39;lxml\u0026#39;) print(soup.find_all(\u0026#34;title\u0026#34;)) # 查找所有的title标签 print(soup.find_all(\u0026#34;p\u0026#34;, \u0026#34;title\u0026#34;)) # 查找p标签中class为title的标签 print(soup.find_all(\u0026#34;a\u0026#34;)) # 查找所有的a标签 print(soup.find_all(id=\u0026#34;link2\u0026#34;)) # 查找id为link2的标签 #[\u0026lt;title\u0026gt;The Dormouse\u0026#39;s story\u0026lt;/title\u0026gt;] #[\u0026lt;p class=\u0026#34;title\u0026#34;\u0026gt;\u0026lt;b\u0026gt;The Dormouse\u0026#39;s story\u0026lt;/b\u0026gt;\u0026lt;/p\u0026gt;] #[\u0026lt;a class=\u0026#34;sister\u0026#34; href=\u0026#34;https://example.com/elsie\u0026#34; id=\u0026#34;link1\u0026#34;\u0026gt;Elsie\u0026lt;/a\u0026gt;, \u0026lt;a class=\u0026#34;sister\u0026#34; href=\u0026#34;https://example.com/lacie\u0026#34; id=\u0026#34;link2\u0026#34;\u0026gt;Lacie\u0026lt;/a\u0026gt;, \u0026lt;a class=\u0026#34;sister\u0026#34; href=\u0026#34;https://example.com/tillie\u0026#34; id=\u0026#34;link3\u0026#34;\u0026gt;Tillie\u0026lt;/a\u0026gt;] #[\u0026lt;a class=\u0026#34;sister\u0026#34; href=\u0026#34;https://example.com/lacie\u0026#34; id=\u0026#34;link2\u0026#34;\u0026gt;Lacie\u0026lt;/a\u0026gt;] find_parent()：只返回最接近的父标签\n1 2 3 4 soup = BeautifulSoup(html_doc, \u0026#39;lxml\u0026#39;) a_string = soup.find(string=\u0026#39;Lacie\u0026#39;) print(a_string.find_parent()) # 查找父节点 #\u0026lt;a class=\u0026#34;sister\u0026#34; href=\u0026#34;https://example.com/lacie\u0026#34; id=\u0026#34;link2\u0026#34;\u0026gt;Lacie\u0026lt;/a\u0026gt; find_parents()：返回所有符合条件的祖先标签，按从近到远的顺序排列\n1 2 3 4 5 6 7 soup = BeautifulSoup(html_doc, \u0026#39;lxml\u0026#39;) a_string = soup.find(string=\u0026#39;Lacie\u0026#39;) print(a_string.find_parents()) # 查找父节点 #[\u0026lt;a class=\u0026#34;sister\u0026#34; href=\u0026#34;https://example.com/lacie\u0026#34; id=\u0026#34;link2\u0026#34;\u0026gt;Lacie\u0026lt;/a\u0026gt;, \u0026lt;p class=\u0026#34;story\u0026#34;\u0026gt;Once upon a time there were three little sisters; and their names were #\u0026lt;a class=\u0026#34;sister\u0026#34; href=\u0026#34;https://example.com/elsie\u0026#34; id=\u0026#34;link1\u0026#34;\u0026gt;Elsie\u0026lt;/a\u0026gt;, #\u0026lt;a class=\u0026#34;sister\u0026#34; href=\u0026#34;https://example.com/lacie\u0026#34; id=\u0026#34;link2\u0026#34;\u0026gt;Lacie\u0026lt;/a\u0026gt; and #and they lived at the bottom of a well.\u0026lt;/p\u0026gt;, \u0026lt;body\u0026gt;....] BeautifulSoup的CSS选择器 我们在写CSS时,标签名不加任何修饰,类名前加点,id名前加#,BeautifulSoup中也可以使用类似的方法来筛选元素。BeautifulSoup中的select()方法允许使用CSS选择器来查找HTML文档元素,其返回一个包含所有匹配元素的列表类似于find_all()方法。\n通过标签名查找：\n1 2 3 soup = BeautifulSoup(html_doc, \u0026#39;lxml\u0026#39;) print(soup.select(\u0026#39;b\u0026#39;)) #[\u0026lt;b\u0026gt;The Dormouse\u0026#39;s story\u0026lt;/b\u0026gt;, \u0026lt;b\u0026gt;\u0026lt;!--This is a comment--\u0026gt;\u0026lt;/b\u0026gt;] 通过类名查找：\n1 2 3 soup = BeautifulSoup(html_doc, \u0026#39;lxml\u0026#39;) print(soup.select(\u0026#39;.title\u0026#39;)) #[\u0026lt;p class=\u0026#34;title\u0026#34;\u0026gt;\u0026lt;b\u0026gt;The Dormouse\u0026#39;s story\u0026lt;/b\u0026gt;\u0026lt;/p\u0026gt;] id名查找：\n1 2 3 soup = BeautifulSoup(html_doc, \u0026#39;lxml\u0026#39;) print(soup.select(\u0026#39;#link1\u0026#39;)) #[\u0026lt;a class=\u0026#34;sister\u0026#34; href=\u0026#34;https://example.com/elsie\u0026#34; id=\u0026#34;link1\u0026#34;\u0026gt;Elsie\u0026lt;/a\u0026gt;] 组合查找：组合查找即与写class时一致,标签名与类名id名进行组合的原理一样\n1 2 3 soup = BeautifulSoup(html_doc, \u0026#39;lxml\u0026#39;) print(soup.select(\u0026#39;p #link1\u0026#39;)) #[\u0026lt;a class=\u0026#34;sister\u0026#34; href=\u0026#34;https://example.com/elsie\u0026#34; id=\u0026#34;link1\u0026#34;\u0026gt;Elsie\u0026lt;/a\u0026gt;] 属性查找：选择具有特定属性或属性值的标签\n简单属性选择器：\n1 2 3 soup = BeautifulSoup(html_doc, \u0026#39;lxml\u0026#39;) print(soup.select(\u0026#34;a[href=\u0026#39;https://example.com/elsie\u0026#39;]\u0026#34;)) # 选择a标签中href属性为https://example.com/elsie的标签 #[\u0026lt;a class=\u0026#34;sister\u0026#34; href=\u0026#34;https://example.com/elsie\u0026#34; id=\u0026#34;link1\u0026#34;\u0026gt;Elsie\u0026lt;/a\u0026gt;] 属性值选择器\n1 2 3 4 5 6 7 8 9 10 11 12 13 \u0026#39;\u0026#39;\u0026#39; 精确匹配:[attribute=\u0026#34;value\u0026#34;] 部分匹配 包含特定值:[attribute~=\u0026#34;value\u0026#34;] 选择属性值包含特定单词的标签。 以特定值开头:[attribute^=\u0026#34;value\u0026#34;] 选择属性值以特定字符串开头的标签 以特定值结尾:[attribute$=\u0026#34;value\u0026#34;] 选择属性值以特定字符串结尾的标签。 包含特定子字符串:[attribute*=\u0026#34;value\u0026#34;] 选择属性值包含特定子字符串的标签 \u0026#39;\u0026#39;\u0026#39; soup = BeautifulSoup(html_doc, \u0026#39;lxml\u0026#39;) print(soup.select(\u0026#39;a[href^=\u0026#34;https://example.com\u0026#34;]\u0026#39;)) # 选择href以https://example.com开头的a标签 #[\u0026lt;a class=\u0026#34;sister\u0026#34; href=\u0026#34;https://example.com/elsie\u0026#34; id=\u0026#34;link1\u0026#34;\u0026gt;Elsie\u0026lt;/a\u0026gt;, \u0026lt;a class=\u0026#34;sister\u0026#34; href=\u0026#34;https://example.com/lacie\u0026#34; id=\u0026#34;link2\u0026#34;\u0026gt;Lacie\u0026lt;/a\u0026gt;, \u0026lt;a class=\u0026#34;sister\u0026#34; href=\u0026#34;https://example.com/tillie\u0026#34; id=\u0026#34;link3\u0026#34;\u0026gt;Tillie\u0026lt;/a\u0026gt;] ","date":"2025-08-05T15:27:58+08:00","image":"https://Entars.github.io/p/beautifulsoup/BeautifulSoup_hu_e96c9b8184c5ae44.png","permalink":"https://Entars.github.io/p/beautifulsoup/","title":"BeautifulSoup"},{"content":"Urllib3 特点： 连接池管理：在客户端和服务器之间复用已有连接，避免每次请求都重新建立新连接，核心是PoolManager,内部维护一个或多个ConnectionPool； 线程安全：适合在多线程环境下进行并发请求； 重试机制：请求失败自动重试； SSL/TLS验证：建立 HTTPS 连接时，客户端校验服务器提供的数字证书是否可信，并通过 TLS 协议完成数据加密通道的协商； 代理支持：在客户端（你）访问目标网站时，通过一个中间服务器（代理服务器）中转请求和响应，而不是直接访问目标网站； 文件上传：支持 multipart 文件上传； 编码处理：自动处理响应内容的编码问题； 核心类与方法： PoolManager：最核心类，负责管理连接池和所有请求。\n1 2 3 4 5 6 7 8 http = urllib3.PoolManager( num_pools = 50, #连接池数量 maxsize = 10, #每个连接池最大连接数 block = True, #连接池满时是否阻塞等待 timeout = 30.0,\t#请求超时时间 retries = 3, #默认重试次数 headers={\u0026#39;User-Agent\u0026#39;:\u0026#39;\u0026#39;} ) GET请求：\n1 2 3 4 5 response = http.request( \u0026#39;GET\u0026#39;, \u0026#39;http://\u0026#39;, fields = {\u0026#39;arg\u0026#39;:\u0026#39;value\u0026#39;} #查询参数 ) POST请求：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 #表单数据 response = http.request( \u0026#39;POST\u0026#39;, \u0026#39;http://\u0026#39;, fields = {\u0026#39;field\u0026#39;:\u0026#39;value\u0026#39;} ) #JSON数据 import json response = http.request( \u0026#39;POST\u0026#39;, \u0026#39;http://\u0026#39;, body = json.dumps({\u0026#39;key\u0026#39;:\u0026#39;value\u0026#39;}).encode(\u0026#39;utf-8\u0026#39;), headers = {\u0026#39;Content-Type\u0026#39;:\u0026#39;application/json\u0026#39;} ) PUT/DELETE请求：\n1 2 3 4 5 6 7 8 9 10 11 12 #PUT请求 response = http.request( \u0026#39;PUT\u0026#39;, \u0026#39;http://\u0026#39;, body = b \u0026#39;data to put\u0026#39; ) #DELETE请求 response = http.request( \u0026#39;DELETE\u0026#39;, \u0026#39;http://\u0026#39; ) 文件上传：\n1 2 3 4 5 6 7 8 9 10 11 with open(\u0026#39;example.txt\u0026#39;,\u0026#39;rb\u0026#39;) as f: file_data = f.read() response = http.request( \u0026#39;POST\u0026#39;, \u0026#39;http://\u0026#39;, fields = { \u0026#39;filefield\u0026#39;:(\u0026#39;example.txt\u0026#39;,file_data,\u0026#39;text/plain\u0026#39;), \u0026#39;description\u0026#39;:\u0026#39;File upload example\u0026#39; } ) 响应处理与重要属性 响应对象属性：\n1 2 3 4 5 6 7 8 9 10 11 12 13 response = http.request(\u0026#39;GET\u0026#39;,\u0026#39;http://exmaple.com\u0026#39;) #状态码 print(response.status) #响应头 print(reponse.headers) #响应体 print(response.data) print(response.data.decode(\u0026#39;utf-8\u0026#39;))#解码为字符串 #重新定向历史 print(response.redirect_location) #消耗时间 print(response.elapsed) 响应内容处理：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 #JSON响应处理 import json json_response = json.loads(response.data.decode(\u0026#39;utf-8\u0026#39;)) #流式响应处理 response = http.request( \u0026#39;GET\u0026#39;, \u0026#39;http://exmaple.com/largefile\u0026#39;, preload_content=False ) try: for chunk in response.stream(1024):#每次读1024字节 process_chunk(chunk) finally: response.release_conn()#释放连接 高级特性与配置 重试机制：\n1 2 3 4 5 6 7 8 from urllib3.util.retry import Retry retry_strategy = Retry( total = 3, #重试总次数 backoff_factor = 1, #重试间隔增长因子 status_forcelist = [500,502,503,504]#指定哪些状态码会触发自动重试 ) http = urllib3.PoolManager(retries = retry_strategy) 超时设置：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 #全局超时 http = urllib3.PoolManager(timeout=2.0) #单个请求超时 response = http.request( \u0026#39;GET\u0026#39;, \u0026#39;http://example.com\u0026#39;, timeout=5.0 ) #分别设置连接和读取超时 response = http.request( \u0026#39;GET\u0026#39;, \u0026#39;http://example.com\u0026#39;, timeout=urllib3.Timeout(connect=2.0, read=10.0) ) SSL/TLS配置：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 #禁用证书验证(不推荐生产环境使用) http = urllib3.PoolManager( cert_reqs=\u0026#39;CERT_NONE\u0026#39;,#不校验证书有效性 assert_hostname=False #不检查证书是否匹配域名 ) #自定义CA证书 http = urllib3.PoolManager( cert_reqs=\u0026#39;CERT_REQUIRED\u0026#39;, ca_certs=\u0026#39;/path/to/certificate.pem\u0026#39;)#自定义CA #客户端证书认证 http = urllib3.PoolManager( #证书文件路径 cert_file=\u0026#39;/path/to/client_cert.pem\u0026#39;, #证书对应私钥文件路径 key_file=\u0026#39;/path/to/client_key.pem\u0026#39;) 代理配置：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 #HTTP代理 http = urllib3.ProxyManager( #代理服务器地址 \u0026#39;http://proxy.example.com:8080/\u0026#39;, #身份认证信息 proxy_headers={\u0026#39;Proxy-Authorization\u0026#39;: \u0026#39;Basic ...\u0026#39;}) #SOCKS代理(需要安装PySocks) pip install pysocks from urllib3.contrib.socks import SOCKSProxyManager proxy = SOCKSProxyManager( \u0026#39;socks5://user:password@127.0.0.1:1080/\u0026#39; ) 性能优化技巧 连接池调优：\n1 2 3 4 5 6 7 #根据应用场景调整连接池参数 http = urllib3.PoolManager( num_pools=10, # 适合大多数应用 maxsize=10, # 每个连接池最大连接数 block=True, # 连接池满时阻塞而非创建新连接 timeout=60.0 # 适当延长超时时间 ) 连接重用：\n1 2 3 4 #使用上下文管理器确保连接正确释放 with http.request(\u0026#39;GET\u0026#39;, \u0026#39;http://example.com\u0026#39;, preload_content=False) as response: process_response(response) #连接自动返回到连接池 批处理请求（使用线程池并发发起请求，并收集响应对象）：\n1 2 3 4 5 6 7 8 9 10 11 from concurrent.futures import ThreadPoolExecutor urls = [\u0026#39;http://example.com/1\u0026#39;, \u0026#39;http://example.com/2\u0026#39;, \u0026#39;http://example.com/3\u0026#39;] def fetch(url): return http.request(\u0026#39;GET\u0026#39;, url) with ThreadPoolExecutor(max_workers=5) as executor: results = list(executor.map(fetch, urls)) 6.常见的应用场景 Web API调用：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 import json from urllib.parse import urlencode base_url = \u0026#34;https://api.example.com/v1\u0026#34; def get_user(user_id): response = http.request( \u0026#39;GET\u0026#39;, f\u0026#34;{base_url}/users/{user_id}\u0026#34;, headers={\u0026#39;Authorization\u0026#39;: \u0026#39;Bearer token123\u0026#39;} ) return json.loads(response.data.decode(\u0026#39;utf-8\u0026#39;)) def search_users(query, limit=10): params = {\u0026#39;q\u0026#39;: query, \u0026#39;limit\u0026#39;: limit} response = http.request( \u0026#39;GET\u0026#39;, f\u0026#34;{base_url}/users/search?{urlencode(params)}\u0026#34; ) return json.loads(response.data.decode(\u0026#39;utf-8\u0026#39;)) 网页抓取：\n1 2 3 4 5 6 7 8 9 10 11 12 13 from bs4 import BeautifulSoup def scrape_website(url): response = http.request(\u0026#39;GET\u0026#39;, url) if response.status == 200: soup = BeautifulSoup(response.data, \u0026#39;html.parser\u0026#39;) # 提取数据... return { \u0026#39;title\u0026#39;: soup.title.string, #遍历所有的a标签，并提取其中的href属性 \u0026#39;links\u0026#39;: [a[\u0026#39;href\u0026#39;] for a in soup.find_all(\u0026#39;a\u0026#39;)] } return None 文件下载：\n1 2 3 4 5 6 7 8 def download_file(url, save_path): with http.request(\u0026#39;GET\u0026#39;, url, preload_content=False) as response: if response.status == 200: with open(save_path, \u0026#39;wb\u0026#39;) as f: for chunk in response.stream(1024): f.write(chunk) return True return False 微服务通信：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 import json def call_service(service_url, method, payload=None): headers = { \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;, \u0026#39;X-Request-ID\u0026#39;: \u0026#39;unique-id-123\u0026#39; } body = json.dumps(payload).encode(\u0026#39;utf-8\u0026#39;) if payload else None response = http.request( method.upper(), service_url, headers=headers, body=body ) if response.status \u0026gt;= 400: raise Exception(f\u0026#34;Service error: {response.status}\u0026#34;) return json.loads(response.data.decode(\u0026#39;utf-8\u0026#39;)) 常见问题 异常处理：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 import urllib3.exceptions try: response = http.request(\u0026#39;GET\u0026#39;, \u0026#39;http://example.com\u0026#39;) except urllib3.exceptions.HTTPError as e: print(f\u0026#34;HTTP错误: {e}\u0026#34;) except urllib3.exceptions.SSLError as e: print(f\u0026#34;SSL错误: {e}\u0026#34;) except urllib3.exceptions.TimeoutError as e: print(f\u0026#34;请求超时: {e}\u0026#34;) except urllib3.exceptions.RequestError as e: print(f\u0026#34;请求错误: {e}\u0026#34;) except Exception as e: print(f\u0026#34;其他错误: {e}\u0026#34;) 调试技巧：\n1 2 3 4 5 6 7 8 9 10 11 #启用调试日志 import logging logging.basicConfig(level=logging.DEBUG) #或者只启用urllib3的调试日志 logger = logging.getLogger(\u0026#39;urllib3\u0026#39;) logger.setLevel(logging.DEBUG) #查看连接池状态 print(http.connection_pool_kw) print(http.pools) 与request库的对比 更底层的控制：直接访问连接池和底层配置 更小的内存占用：没有 requests 的额外抽象层 更早的错误检测：在请求发送前就能检测到某些问题 更灵活的流处理：对大型文件或流式API更友好 大多数情况用request，精细控制或高性能用urllib3 一些问题 Q 当我向https://www.xiaohongshu.com/发起请求失败时，urllib3会自动重新定向到https://www.xiaohongshu.com/explore，这是为什么，程序为什么会知道https://www.xiaohongshu.com/explore是正确的 A 不是urllib3“知道正确地址”，重新定向是服务端的行为，当向https://www.xiaohongshu.com/发起请求后，小红书的服务器会返回一个HTTP 301或302响应，将正确的网址返回给客户端，同时urllib会默认自动跟随重定向。 Q JSON响应处理和流式响应处理的区别 A 对于JSON响应处理，response.data一次性读取全部内容，再将其解析为Python对象；流式响应处理，不会把整个响应体加载进内存。JSON响应处理适用于REST API、结构化数据；流式响应处理适用于下载大文件、音视频流等场景 Q 线程与并发的关系 A 线程是并发的一种实现方式，但并发并不限于线程，线程像“工人”，并发像“工人排班的调度方式”，一个线程处理一个任务，多个线程合作是实现 一些测试代码 一些库导入\n1 2 3 4 5 6 7 import urllib3 import threading import time from urllib3.exceptions import EmptyPoolError import json import logging logging.basicConfig(level=logging.DEBUG) 测试连接复用：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 # 创建连接池，最大连接数设置为 1（便于观察复用） http = urllib3.HTTPConnectionPool(\u0026#34;httpbin.org\u0026#34;, maxsize=1) # 第一次请求 print(\u0026#34;第1次请求\u0026#34;) r1 = http.request(\u0026#34;GET\u0026#34;, \u0026#34;/get\u0026#34;) print(\u0026#34;响应状态码:\u0026#34;, r1.status) # 暂停 1 秒 time.sleep(1) # 第二次请求 print(\u0026#34;\\n第2次请求\u0026#34;) r2 = http.request(\u0026#34;GET\u0026#34;, \u0026#34;/get\u0026#34;) print(\u0026#34;响应状态码:\u0026#34;, r2.status) \u0026#39;\u0026#39;\u0026#39; #通过如下方式，检查底层 socket 是否复用 print(\u0026#34;连接池中连接数量:\u0026#34;, len(http.pool)) for conn in http.pool: print(\u0026#34;连接对象:\u0026#34;, conn) \u0026#39;\u0026#39;\u0026#39; \u0026#39;\u0026#39;\u0026#39; #测试“不复用的情况”，可以设置 retries=0，并且让服务器关闭连接 r = http.request(\u0026#34;GET\u0026#34;, \u0026#34;/get\u0026#34;, headers={\u0026#34;Connection\u0026#34;: \u0026#34;close\u0026#34;}) \u0026#39;\u0026#39;\u0026#39; 测试限制连接数+阻塞模式:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 # 设置连接池为 maxsize=1，block=False http = urllib3.HTTPConnectionPool(\u0026#34;httpbin.org\u0026#34;, maxsize=1, block=False) # 请求函数：保持连接占用一段时间 def make_request(name): print(f\u0026#34;[{name}] 准备发送请求\u0026#34;) try: r = http.request(\u0026#34;GET\u0026#34;, \u0026#34;/delay/5\u0026#34;, preload_content=False, timeout=10.0) print(f\u0026#34;[{name}] 已发送请求，延迟读取内容\u0026#34;) time.sleep(6) # 保持连接被占用 r.release_conn() # 主动释放连接 print(f\u0026#34;[{name}] 释放连接\u0026#34;) except EmptyPoolError as e: print(f\u0026#34;[{name}] 连接池耗尽异常: {e}\u0026#34;) except Exception as e: print(f\u0026#34;[{name}] 其他异常: {e}\u0026#34;) # 第一个线程：占住唯一连接 t1 = threading.Thread(target=make_request, args=(\u0026#34;线程1\u0026#34;,)) # 第二个线程：争抢连接 t2 = threading.Thread(target=make_request, args=(\u0026#34;线程2\u0026#34;,)) t1.start() time.sleep(0.2) # 保证线程1先占住连接 t2.start() t1.join() t2.join() \u0026#39;\u0026#39;\u0026#39; 连接池已满且不允许阻塞新的请求时，新的连接请求将会被丢弃。这意味着连接池无法处理所有的请求，可能会导致一些请求失败或被延迟处理。 所以在触发连接池已满的情况下不会报错且能正常运行，但在DEBUG日志下会有警告。 \u0026#39;\u0026#39;\u0026#39; 验证连接池（num_pool）作用:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 \u0026#39;num_pool控制的是最多允许缓存多少个不同主机的连接池，每一个host:port组合会对应一个connectionpool实例，当请求的host数超过num_pools时，旧的连接池将会被清理(LRU策略)\u0026#39; # 创建连接池管理器，最多2个主机连接池 http = urllib3.PoolManager(num_pools=2, maxsize=1) hosts = [ \u0026#34;http://httpbin.org/get\u0026#34;, \u0026#34;https://www.xiaohongshu.com/explore\u0026#34;, \u0026#34;http://baidu.com\u0026#34; # 任意有效网址也可 ] def fetch(url): try: r = http.request(\u0026#34;GET\u0026#34;, url) print(f\u0026#34;访问 {url} 状态码: {r.status}\u0026#34;) except Exception as e: print(f\u0026#34;访问 {url} 出错: {e}\u0026#34;) # 访问3个不同host，超出num_pools限制 for i in range(2): for url in hosts: fetch(url) time.sleep(0.5) \u0026#39;\u0026#39;\u0026#39; 可以观察到每次发送请求前都得先建立连接，这是因为只能允许存在两个连接池，当继续往里添加连接池时，会把旧的清理，使得再次连接时需要重新建立连接 \u0026#39;\u0026#39;\u0026#39; ","date":"2025-08-05T12:10:50+08:00","image":"https://Entars.github.io/p/urllib3/urllib3_hu_c05e892ee201c66c.png","permalink":"https://Entars.github.io/p/urllib3/","title":"Urllib3"},{"content":"正文测试 而这些并不是完全重要，更加重要的问题是， 带着这些问题，我们来审视一下学生会退会。 既然如何， 对我个人而言，学生会退会不仅仅是一个重大的事件，还可能会改变我的人生。 我们不得不面对一个非常尴尬的事实，那就是， 可是，即使是这样，学生会退会的出现仍然代表了一定的意义。 学生会退会，发生了会如何，不发生又会如何。 经过上述讨论， 生活中，若学生会退会出现了，我们就不得不考虑它出现了的事实。 学生会退会，到底应该如何实现。 这样看来， 在这种困难的抉择下，本人思来想去，寝食难安。 对我个人而言，学生会退会不仅仅是一个重大的事件，还可能会改变我的人生。 就我个人来说，学生会退会对我的意义，不能不说非常重大。 莎士比亚曾经提到过，人的一生是短的，但如果卑劣地过这一生，就太长了。这似乎解答了我的疑惑。 莫扎特说过一句富有哲理的话，谁和我一样用功，谁就会和我一样成功。这启发了我， 对我个人而言，学生会退会不仅仅是一个重大的事件，还可能会改变我的人生。 学生会退会，到底应该如何实现。 一般来说， 从这个角度来看， 这种事实对本人来说意义重大，相信对这个世界也是有一定意义的。 在这种困难的抉择下，本人思来想去，寝食难安。 了解清楚学生会退会到底是一种怎么样的存在，是解决一切问题的关键。 一般来说， 生活中，若学生会退会出现了，我们就不得不考虑它出现了的事实。 问题的关键究竟为何？ 而这些并不是完全重要，更加重要的问题是。\n奥斯特洛夫斯基曾经说过，共同的事业，共同的斗争，可以使人们产生忍受一切的力量。　带着这句话，我们还要更加慎重的审视这个问题： 一般来讲，我们都必须务必慎重的考虑考虑。 既然如此， 这种事实对本人来说意义重大，相信对这个世界也是有一定意义的。 带着这些问题，我们来审视一下学生会退会。 我认为， 我认为， 在这种困难的抉择下，本人思来想去，寝食难安。 问题的关键究竟为何？ 每个人都不得不面对这些问题。 在面对这种问题时， 要想清楚，学生会退会，到底是一种怎么样的存在。 我认为， 既然如此， 每个人都不得不面对这些问题。 在面对这种问题时， 那么， 我认为， 学生会退会因何而发生。\n引用 思念是最暖的忧伤像一双翅膀\n让我停不了飞不远在过往游荡\n不告而别的你 就算为了我着想\n这么沉痛的呵护 我怎么能翱翔\n最暖的憂傷 - 田馥甄\n图片 1 2 3 ![Photo by Florian Klauer on Unsplash](florian-klauer-nptLmg6jqDo-unsplash.jpg) ![Photo by Luca Bravo on Unsplash](luca-bravo-alS7ewQ41M8-unsplash.jpg) ![Photo by Helena Hertz on Unsplash](helena-hertz-wWZzXlDpMog-unsplash.jpg) ![Photo by Hudai Gayiran on Unsplash](hudai-gayiran-3Od_VKcDEAA-unsplash.jpg) 相册语法来自 Typlog\n","date":"2020-09-09T00:00:00Z","image":"https://Entars.github.io/p/test-chinese/helena-hertz-wWZzXlDpMog-unsplash_hu_2307260c751d0e0b.jpg","permalink":"https://Entars.github.io/p/test-chinese/","title":"Chinese Test"},{"content":"This article offers a sample of basic Markdown syntax that can be used in Hugo content files, also it shows whether basic HTML elements are decorated with CSS in a Hugo theme.\nHeadings The following HTML \u0026lt;h1\u0026gt;—\u0026lt;h6\u0026gt; elements represent six levels of section headings. \u0026lt;h1\u0026gt; is the highest section level while \u0026lt;h6\u0026gt; is the lowest.\nH1 H2 H3 H4 H5 H6 Paragraph Xerum, quo qui aut unt expliquam qui dolut labo. Aque venitatiusda cum, voluptionse latur sitiae dolessi aut parist aut dollo enim qui voluptate ma dolestendit peritin re plis aut quas inctum laceat est volestemque commosa as cus endigna tectur, offic to cor sequas etum rerum idem sintibus eiur? Quianimin porecus evelectur, cum que nis nust voloribus ratem aut omnimi, sitatur? Quiatem. Nam, omnis sum am facea corem alique molestrunt et eos evelece arcillit ut aut eos eos nus, sin conecerem erum fuga. Ri oditatquam, ad quibus unda veliamenimin cusam et facea ipsamus es exerum sitate dolores editium rerore eost, temped molorro ratiae volorro te reribus dolorer sperchicium faceata tiustia prat.\nItatur? Quiatae cullecum rem ent aut odis in re eossequodi nonsequ idebis ne sapicia is sinveli squiatum, core et que aut hariosam ex eat.\nBlockquotes The blockquote element represents content that is quoted from another source, optionally with a citation which must be within a footer or cite element, and optionally with in-line changes such as annotations and abbreviations.\nBlockquote without attribution Tiam, ad mint andaepu dandae nostion secatur sequo quae. Note that you can use Markdown syntax within a blockquote.\nBlockquote with attribution Don\u0026rsquo;t communicate by sharing memory, share memory by communicating.\n— Rob Pike1\nTables Tables aren\u0026rsquo;t part of the core Markdown spec, but Hugo supports supports them out-of-the-box.\nName Age Bob 27 Alice 23 Inline Markdown within tables Italics Bold Code italics bold code A B C D E F Lorem ipsum dolor sit amet, consectetur adipiscing elit. Phasellus ultricies, sapien non euismod aliquam, dui ligula tincidunt odio, at accumsan nulla sapien eget ex. Proin eleifend dictum ipsum, non euismod ipsum pulvinar et. Vivamus sollicitudin, quam in pulvinar aliquam, metus elit pretium purus Proin sit amet velit nec enim imperdiet vehicula. Ut bibendum vestibulum quam, eu egestas turpis gravida nec Sed scelerisque nec turpis vel viverra. Vivamus vitae pretium sapien Code Blocks Code block with backticks 1 2 3 4 5 6 7 8 9 10 \u0026lt;!doctype html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;utf-8\u0026#34;\u0026gt; \u0026lt;title\u0026gt;Example HTML5 Document\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p\u0026gt;Test\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Code block indented with four spaces \u0026lt;!doctype html\u0026gt; \u0026lt;html lang=\u0026quot;en\u0026quot;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026quot;utf-8\u0026quot;\u0026gt; \u0026lt;title\u0026gt;Example HTML5 Document\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p\u0026gt;Test\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Code block with Hugo\u0026rsquo;s internal highlight shortcode 1 2 3 4 5 6 7 8 9 10 \u0026lt;!doctype html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;utf-8\u0026#34;\u0026gt; \u0026lt;title\u0026gt;Example HTML5 Document\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p\u0026gt;Test\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Diff code block 1 2 3 4 5 [dependencies.bevy] git = \u0026#34;https://github.com/bevyengine/bevy\u0026#34; rev = \u0026#34;11f52b8c72fc3a568e8bb4a4cd1f3eb025ac2e13\u0026#34; - features = [\u0026#34;dynamic\u0026#34;] + features = [\u0026#34;jpeg\u0026#34;, \u0026#34;dynamic\u0026#34;] List Types Ordered List First item Second item Third item Unordered List List item Another item And another item Nested list Fruit Apple Orange Banana Dairy Milk Cheese Other Elements — abbr, sub, sup, kbd, mark GIF is a bitmap image format.\nH2O\nXn + Yn = Zn\nPress CTRL + ALT + Delete to end the session.\nMost salamanders are nocturnal, and hunt for insects, worms, and other small creatures.\nHyperlinked image The above quote is excerpted from Rob Pike\u0026rsquo;s talk during Gopherfest, November 18, 2015.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"2019-03-11T00:00:00Z","image":"https://Entars.github.io/p/markdown-syntax-guide/pawel-czerwinski-8uZPynIu-rQ-unsplash_hu_e95a4276bf860a84.jpg","permalink":"https://Entars.github.io/p/markdown-syntax-guide/","title":"Markdown Syntax Guide"},{"content":"Lorem est tota propiore conpellat pectoribus de pectora summo.\nRedit teque digerit hominumque toris verebor lumina non cervice subde tollit usus habet Arctonque, furores quas nec ferunt. Quoque montibus nunc caluere tempus inhospita parcite confusaque translucet patri vestro qui optatis lumine cognoscere flos nubis! Fronde ipsamque patulos Dryopen deorum.\nExierant elisi ambit vivere dedere Duce pollice Eris modo Spargitque ferrea quos palude Rursus nulli murmur; hastile inridet ut ab gravi sententia! Nomine potitus silentia flumen, sustinet placuit petis in dilapsa erat sunt. Atria tractus malis.\nComas hunc haec pietate fetum procerum dixit Post torum vates letum Tiresia Flumen querellas Arcanaque montibus omnes Quidem et Vagus elidunt The Van de Graaf Canon\nMane refeci capiebant unda mulcebat Victa caducifer, malo vulnere contra dicere aurato, ludit regale, voca! Retorsit colit est profanae esse virescere furit nec; iaculi matertera et visa est, viribus. Divesque creatis, tecta novat collumque vulnus est, parvas. Faces illo pepulere tempus adest. Tendit flamma, ab opes virum sustinet, sidus sequendo urbis.\nIubar proles corpore raptos vero auctor imperium; sed et huic: manus caeli Lelegas tu lux. Verbis obstitit intus oblectamina fixis linguisque ausus sperare Echionides cornuaque tenent clausit possit. Omnia putatur. Praeteritae refert ausus; ferebant e primus lora nutat, vici quae mea ipse. Et iter nil spectatae vulnus haerentia iuste et exercebat, sui et.\nEurytus Hector, materna ipsumque ut Politen, nec, nate, ignari, vernum cohaesit sequitur. Vel mitis temploque vocatus, inque alis, oculos nomen non silvis corpore coniunx ne displicet illa. Crescunt non unus, vidit visa quantum inmiti flumina mortis facto sic: undique a alios vincula sunt iactata abdita! Suspenderat ego fuit tendit: luna, ante urbem Propoetides parte.\n","date":"2019-03-09T00:00:00Z","image":"https://Entars.github.io/p/placeholder-text/matt-le-SJSpo9hQf7s-unsplash_hu_c1ca39d792aee4ab.jpg","permalink":"https://Entars.github.io/p/placeholder-text/","title":"Placeholder Text"},{"content":"Mathematical notation in a Hugo project can be enabled by using third party JavaScript libraries.\nIn this example we will be using KaTeX\nCreate a partial under /layouts/partials/math.html Within this partial reference the Auto-render Extension or host these scripts locally. Include the partial in your templates like so: 1 2 3 {{ if or .Params.math .Site.Params.math }} {{ partial \u0026#34;math.html\u0026#34; . }} {{ end }} To enable KaTeX globally set the parameter math to true in a project\u0026rsquo;s configuration To enable KaTeX on a per page basis include the parameter math: true in content files Note: Use the online reference of Supported TeX Functions\nExamples Inline math: $\\varphi = \\dfrac{1+\\sqrt5}{2}= 1.6180339887…$\nBlock math: $$ \\varphi = 1+\\frac{1} {1+\\frac{1} {1+\\frac{1} {1+\\cdots} } } $$","date":"2019-03-08T00:00:00Z","permalink":"https://Entars.github.io/p/math-typesetting/","title":"Math Typesetting"},{"content":"Emoji can be enabled in a Hugo project in a number of ways.\nThe emojify function can be called directly in templates or Inline Shortcodes.\nTo enable emoji globally, set enableEmoji to true in your site\u0026rsquo;s configuration and then you can type emoji shorthand codes directly in content files; e.g.\n🙈 :see_no_evil: 🙉 :hear_no_evil: 🙊 :speak_no_evil:\nThe Emoji cheat sheet is a useful reference for emoji shorthand codes.\nN.B. The above steps enable Unicode Standard emoji characters and sequences in Hugo, however the rendering of these glyphs depends on the browser and the platform. To style the emoji you can either use a third party emoji font or a font stack; e.g.\n1 2 3 .emoji { font-family: Apple Color Emoji, Segoe UI Emoji, NotoColorEmoji, Segoe UI Symbol, Android Emoji, EmojiSymbols; } ","date":"2019-03-05T00:00:00Z","image":"https://Entars.github.io/p/emoji-support/the-creative-exchange-d2zvqp3fpro-unsplash_hu_27b8954607cdb515.jpg","permalink":"https://Entars.github.io/p/emoji-support/","title":"Emoji Support"}]