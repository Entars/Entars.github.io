[{"content":"获取Cookie ​\t起初在进入到岗位页面时，发现无需登录也能访问岗位列表，因此打算直接从网络中找到返回工作列表的请求，通过侧边栏搜索相关字段，定位请求位置:\n获取其中的请求URL和Cookie等参数，构造请求头并发起请求。在此过程中发现请求URL中最后一段数字是在不断变化的：\n查询网页内部的js文件发现，该串数字和时间有关，因此大胆猜测这段数字可能是时间戳，采用代码测试：\n1 2 3 4 5 6 import time def generate_timestamp(): \u0026#34;\u0026#34;\u0026#34;生成13位时间戳\u0026#34;\u0026#34;\u0026#34; return str(int(time.time() * 1000)) print(generate_timestamp()) #output-\u0026gt;1755764369564 代码测试结果与URL的末端数字相似，因此得证。但在爬取的过程中，发现在未登录状态下只能浏览前10页内容。因此需要解决登录问题，找到相关请求：\n根据相关字段向该URL提交表单并获取登录后的Cookie，但用Python获取Cookie后发现依旧无法获取，返回的结果：\n一开始我怀疑是获取的Cookie有问题，因此使用代码分析直接从网页上获取的Cookie的变化，试图找到规律：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 from difflib import SequenceMatcher #将Cookie进行切割 def Cookie_split(Cookie): fragments = Cookie.split(\u0026#34;; \u0026#34;) dic = {} for fragment in fragments: key,value = fragment.split(\u0026#39;=\u0026#39;,1) dic[key] = value return dic def compare_strings(str1,str2): matcher = SequenceMatcher(None,str1,str2) for op , i1 , i2 ,j1 ,j2 in matcher.get_opcodes(): if op == \u0026#39;replace\u0026#39;: print(f\u0026#34;在位置{i1}替换了“{str1[i1:i2]}”为“{str2[j1:j2]}”\u0026#34;) elif op == \u0026#39;delete\u0026#39;: print(f\u0026#34;在位置{i1}删除了“{str1[i1:i2]}”\u0026#34;) elif op == \u0026#39;insert\u0026#39;: print(f\u0026#34;在位置{i1}插入了“{str1[i1:i2]}”\u0026#34;) #Cookie比较 def Cookie_compare(Cookie1,Cookie2): \u0026#34;\u0026#34;\u0026#34;比较两个Cookie的差别\u0026#34;\u0026#34;\u0026#34; cookie_dic1 = Cookie_split(Cookie1) cookie_dic2 = Cookie_split(Cookie2) key_list1 = cookie_dic1.keys() key_list2 = cookie_dic2.keys() print(key_list1) print(key_list2) # Cookie中属性的比较 diff = list(set(key_list1) ^ set(key_list2)) if diff == []: print(\u0026#34;两个Cookie在属性上一致\u0026#34;) for key in key_list1: print(f\u0026#34;|--------------------------{key}的差异点-------------------------|\u0026#34;) compare_strings(cookie_dic1[key],cookie_dic2[key]) print(f\u0026#34;|--------------------------------------------------------------|\u0026#34;) else: print(f\u0026#34;两个Cookie的属性差异：{diff}\u0026#34;) same_attribute = list(set(key_list1) \u0026amp; set(key_list2)) for key in same_attribute: print(f\u0026#34;|--------------------------{key}的差异点-------------------------|\u0026#34;) compare_strings(cookie_dic1[key],cookie_dic2[key]) print(f\u0026#34;|--------------------------------------------------------------|\u0026#34;) if __name__ == \u0026#34;__main__\u0026#34;: Cookie_compare(Cookie6,Cookie7) 在比较的过程中发现，当网页保持登录状态时，从网页获取的Cookie可以正常使用，而当关闭网页后，之前获取的Cookie就失效了，因此怀疑整个流程如图：\n所以如果要保持Cookie的有效性，就必须保持登录状态。 因此考虑使用Selenium进行模拟登录，再从其中获取组成Cookie所需的相关字段。事实上，能直接获取的字段并不全，因此通过上面的比较代码，找到Cookie中的不变（有时不变）部分和变动（每次登录都会改变）部分，幸运的是变动部分都能在直接获取的字段中找到。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 def keep_connect(chrome_location,username,password): \u0026#34;\u0026#34;\u0026#34;使用selenium模拟登录，获取Cookie，并维持登录状态防止服务器端将Cookie注销\u0026#34;\u0026#34;\u0026#34; options = webdriver.ChromeOptions() options.binary_location = chrome_location options.add_argument(\u0026#34;ignore-certificate-errors\u0026#34;) options.add_experimental_option(\u0026#34;detach\u0026#34;, True) driver = webdriver.Chrome(options=options, executable_path=\u0026#39;../chromedriver.exe\u0026#39;) login_url = \u0026#39;https://account.chsi.com.cn/passport/login?service=https://www.ncss.cn/student/connect/chsi\u0026amp;entrytype=stu\u0026#39; driver.get(login_url) time.sleep(1) acc_input = driver.find_element(By.XPATH, r\u0026#39;/html/body/div/div[2]/div[2]/div/div[2]/form/div[1]/input\u0026#39;) acc_input.send_keys(username) pwd_input = driver.find_element(By.XPATH, f\u0026#39;/html/body/div/div[2]/div[2]/div/div[2]/form/div[2]/input\u0026#39;) pwd_input.send_keys(password) driver.find_element(By.XPATH, r\u0026#39;/html/body/div/div[2]/div[2]/div/div[2]/form/div[4]\u0026#39;).click() element = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.XPATH, r\u0026#34;/html/body/div/span/span/a[1]\u0026#34;))) element.click() dic = {} for i in driver.get_cookies(): dic[i[\u0026#39;name\u0026#39;]] = i[\u0026#39;value\u0026#39;] return dic def Cookie_concat(got_property): \u0026#34;\u0026#34;\u0026#34;将从keep_connect函数中获取到的Cookie数据进行重新包装\u0026#34;\u0026#34;\u0026#34; changed_property_list = [\u0026#39;SESSION\u0026#39;, \u0026#39;Hm_lpvt_378ff17a1ac046691cf78376632d1ed4\u0026#39;, \u0026#39;_ga\u0026#39;,\u0026#39;_ga_6CXWRD3K0D\u0026#39;] no_change = { \u0026#39;_ga_1ESVLDHDYL\u0026#39;: \u0026#39;GS1.1.1726503021.3.0.1726503021.0.0.0\u0026#39;, \u0026#39;_abfpc\u0026#39;: \u0026#39;d7760a7e8d00953eaa18b111273b00d6ad83f1cc_2.0\u0026#39;, \u0026#39;cna\u0026#39;: \u0026#39;eac1e178d67b00529df2b287b5842f2c\u0026#39;, \u0026#39;_gid\u0026#39;: \u0026#39;GA1.2.673435621.1755336171\u0026#39;, \u0026#39;aliyungf_tc\u0026#39;: \u0026#39;bfc4565f794c9e6ac253a5c09f4a937f2afb69e01fc7b413d486aaeaadf06249\u0026#39;, \u0026#39;XSRF-CCKTOKEN\u0026#39;: \u0026#39;a12b3436b08549ba2f33a4aefa9a1454\u0026#39;, \u0026#39;CHSICC_CLIENTFLAGNCSS\u0026#39;: \u0026#39;aba5b818eaa8bc94d6fb1ddf17d1df4f\u0026#39;, \u0026#39;CHSICC01\u0026#39;: \u0026#39;!DzrVNB/pHD1H78bzYxYLahOzddj6Y4XQ6NJ5RnOPIOyzHzKixC+5X5WINIjztT+S4x5PGaf/cowaI/Q=\u0026#39;, \u0026#39;CHSICC_CLIENTFLAGSTUDENT\u0026#39;: \u0026#39;5d0ab9cce044f18a699886e7d6705555\u0026#39;, \u0026#39;Hm_lvt_378ff17a1ac046691cf78376632d1ed4\u0026#39;: \u0026#39;1754926580,1754968150,1755336169,1755411478\u0026#39;, \u0026#39;HMACCOUNT\u0026#39;: \u0026#39;CEB955474E107530\u0026#39;, \u0026#39;acw_tc\u0026#39;: \u0026#39;ac11000117554230396983911ee7b259a823321b52a20a878c0b42ee677273\u0026#39;, \u0026#39;_gat_gtag_UA_105074615_1\u0026#39;: \u0026#39;1\u0026#39; } property_all = no_change.copy() for i in changed_property_list: property_all[i] = got_property[i] property_rank = [\u0026#39;SESSION\u0026#39;,\u0026#39;_ga_1ESVLDHDYL\u0026#39;,\u0026#39;_abfpc\u0026#39;,\u0026#39;cna\u0026#39;,\u0026#39;_gid\u0026#39;,\u0026#39;aliyungf_tc\u0026#39;,\u0026#39;acw_tc\u0026#39;,\u0026#39;XSRF-CCKTOKEN\u0026#39;,\u0026#39;CHSICC_CLIENTFLAGNCSS\u0026#39;,\u0026#39;CHSICC01\u0026#39;,\u0026#39;CHSICC_CLIENTFLAGSTUDENT\u0026#39;,\u0026#39;Hm_lvt_378ff17a1ac046691cf78376632d1ed4\u0026#39;,\u0026#39;HMACCOUNT\u0026#39;,\u0026#39;_gat_gtag_UA_105074615_1\u0026#39;,\u0026#39;Hm_lpvt_378ff17a1ac046691cf78376632d1ed4\u0026#39;,\u0026#39;_ga\u0026#39;,\u0026#39;_ga_6CXWRD3K0D\u0026#39;] Cookie=\u0026#39;\u0026#39; for i in property_rank: Cookie = Cookie + i + \u0026#39;=\u0026#39; + property_all[i] + \u0026#39;; \u0026#39; print(\u0026#39;完成Cookie创建\u0026#39;) return Cookie def simulate_login_get_cookie(chrome_location,username,password): \u0026#34;\u0026#34;\u0026#34;将模拟登录部分和包装Cookie部分整合在一起\u0026#34;\u0026#34;\u0026#34; dic = keep_connect(chrome_location=chrome_location,username=username,password=password) return Cookie_concat(dic) 获取数据 ​\t数据分为两类，一类是岗位列表数据，另一类是岗位详情页数据。\n岗位列表数据 ​\t岗位列表数据即如图所示：\n该类数据可以直接从返回的响应中的\u0026quot;data\u0026quot;\u0026ndash;\u0026gt;\u0026ldquo;list\u0026quot;中找到，相关代码为：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 def get_job_data(http,Cookie,page=1,page_size=10): \u0026#34;\u0026#34;\u0026#34;获取岗位数据\u0026#34;\u0026#34;\u0026#34; baseurl = \u0026#34;https://www.ncss.cn/student/jobs/jobslist/ajax/\u0026#34; #构造请求头 headers = { \u0026#39;User-Agent\u0026#39;:ua.random, \u0026#39;Connection\u0026#39;:\u0026#39;keep-alive\u0026#39;, \u0026#39;Cookie\u0026#39;:Cookie, \u0026#34;Accept\u0026#34;: \u0026#34;application/json,*/*\u0026#34;, \u0026#39;Referer\u0026#39;:\u0026#34;https://account.chsi.com.cn/passport/login?service=https://job.ncss.cn/student/connect/chsi\u0026amp;entrytype=stu\u0026#34; } # 查询参数 params = { \u0026#34;jobType\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;areaCode\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;jobName\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;monthPay\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;industrySectors\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;property\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;categoryCode\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;memberLevel\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;recruitType\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;offset\u0026#34;: page, \u0026#34;limit\u0026#34;: page_size, \u0026#34;keyUnits\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;degreeCode\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;sourcesName\u0026#34;: \u0026#34;0\u0026#34;, \u0026#34;sourcesType\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;_\u0026#34;: generate_timestamp() # 动态时间戳 } try: resp = http.request( method=\u0026#39;GET\u0026#39;, url=baseurl, fields=params, headers=headers ) if resp.status in [403,401]: print(f\u0026#34;请求第{page}页时登录过期\u0026#34;) return None elif resp.status == 200: data = json.loads(resp.data.decode(\u0026#39;UTF-8\u0026#39;)) return data[\u0026#39;data\u0026#39;][\u0026#39;list\u0026#39;] else: print(f\u0026#34;请求第{page}页时错误，状态码{resp.status}\u0026#34;) return None except Exception as e: print(f\u0026#34;请求第{page}页时发生错误:{e}\u0026#34;) return None 详情页数据 ​\t详情页数据主要为详情页中的岗位介绍部分：\n该部分主要是使用BeautifulSoup对返回的结果进行解析，找到相关属性内容的位置并进行解析，代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 def fetch_detail_page(job_id,http,Cookie): \u0026#34;\u0026#34;\u0026#34;获取详情页数据\u0026#34;\u0026#34;\u0026#34; headers = { \u0026#39;User-Agent\u0026#39;:ua.random, \u0026#39;Cookie\u0026#39;:Cookie, \u0026#39;Referer\u0026#39;:\u0026#39;https://job.ncss.cn/student/jobs/index.html\u0026#39; } detail_url = f\u0026#34;https://www.ncss.cn/student/jobs/{job_id}/detail.html\u0026#34; try: resp = http.request( \u0026#39;GET\u0026#39;, detail_url, headers = headers ) if resp.status == 200: return resp.data.decode(\u0026#39;utf-8\u0026#39;) elif resp.status in [401,403]: print(f\u0026#34;获取{job_id}详情页时登录过期\u0026#34;) return None else: print(f\u0026#34;请求失败，状态码{resp.status}\u0026#34;) return None except Exception as e: print(f\u0026#34;请求失败，{str(e)}\u0026#34;) return None def pares_detail_job_info(html,job_info): \u0026#34;\u0026#34;\u0026#34;将爬取到的详情页进行解析，获取“岗位介绍”部分的数据\u0026#34;\u0026#34;\u0026#34; soup = BeautifulSoup(html,\u0026#39;html.parser\u0026#39;) job_detail_describe_div =soup.find(name=\u0026#39;pre\u0026#39;,attrs={\u0026#39;class\u0026#39;:\u0026#34;mainContent mainContent\u0026#34;}) job_detail_describe = job_detail_describe_div.getText() job_info.update({ \u0026#34;岗位介绍\u0026#34;:job_detail_describe }) return job_info 效率与反爬 ​\t考虑到爬取的数据量稍多，因此使用Urllib3进行分布式爬取，一个主进程，两个工作线程（岗位列表和详情页），采用消费者-生产者模型，同时使用fake_useragent模块不断改变User-Agent的值，代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 def process_manager(total_pages,http,Cookie,num_list_workers=2,num_detail_workers=8,output_address=\u0026#34;\u0026#34;): \u0026#34;\u0026#34;\u0026#34;进程管理器（主进程）\u0026#34;\u0026#34;\u0026#34; global enqueued_jobs_count start_time = time.time() print(f\u0026#34;开始爬取任务，总页数: {total_pages}\u0026#34;) #创建Manager with Manager() as manager: #创建队列 list_page_queue = manager.Queue() detail_task_queue = manager.Queue(maxsize=5000) result_queue = manager.Queue() #添加列表页任务 for page in range(1,total_pages+1): list_page_queue.put(page) expected_total_list_pages = total_pages expected_total_detail_jobs = total_pages * 10 #进度条 list_pbar = tqdm(total=expected_total_list_pages,desc=\u0026#39;列表页\u0026#39;,unit=\u0026#39;页\u0026#39;,dynamic_ncols=True,file=sys.stdout) detail_pbar = tqdm(total=expected_total_detail_jobs,desc=\u0026#39;详情页\u0026#39;,unit=\u0026#39;项\u0026#39;,dynamic_ncols=True,file=sys.stdout) #创建并启动线程 threads = [] #列表页工作线程 for i in range(num_list_workers): t = threading.Thread( target=list_page_worker, args=(list_page_queue,http,Cookie,detail_task_queue,list_pbar), name=f\u0026#34;ListWorker-{i+1}\u0026#34;, daemon=True ) t.start() threads.append(t) #详情页工作线程 for i in range(num_detail_workers): t = threading.Thread( target=detail_page_worker, args=(detail_task_queue,result_queue,http,Cookie,detail_pbar), name=f\u0026#34;DetailWorker-{i+1}\u0026#34;, daemon=True ) t.start() threads.append(t) #结果写入线程 writer_thread = threading.Thread( target=result_writer, args=(result_queue,output_address), name=\u0026#34;ResultWriter\u0026#34;, daemon=True ) writer_thread.start() #等待列表页队列为空（等待完成列表页任务） list_page_queue.join() #通知列表页任务线程退出 for _ in range(num_list_workers): list_page_queue.put(None) print(f\u0026#34;所有列表页任务已完成,{num_list_workers}个工作线程已退出\u0026#34;) #调整详情任务总数 with progress_lock: actual_enqueued = enqueued_jobs_count if actual_enqueued \u0026gt; 0 and actual_enqueued \u0026lt; detail_pbar.total: detail_pbar.total = actual_enqueued detail_pbar.refresh() #等待详情页任务队列为空（详情页任务完成） detail_task_queue.join() #通知详情页工作线程退出 for _ in range(num_detail_workers): detail_task_queue.put(None) print(f\u0026#34;所有详情页任务已完成,{num_detail_workers}个工作线程已退出\u0026#34;) #通知结果写入线程退出 result_queue.put(None) for t in threads: t.join(timeout=10) writer_thread.join(timeout=10) elapsed_time = time.time() - start_time list_pbar.close() detail_pbar.close() print(f\u0026#34;爬取完成! 总耗时: {elapsed_time:.2f} 秒\u0026#34;) 项目流程图 完整代码 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 import queue import random import threading import time from multiprocessing import Manager from bs4 import BeautifulSoup import urllib3 import json from tqdm.auto import tqdm import sys from fake_useragent import UserAgent from selenium import webdriver from selenium.webdriver.common.by import By from selenium.webdriver.support import expected_conditions as EC from selenium.webdriver.support.ui import WebDriverWait #抑制认证警告 urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning) def keep_connect(chrome_location,username,password): \u0026#34;\u0026#34;\u0026#34;使用selenium模拟登录，获取Cookie，并维持登录状态防止服务器端将Cookie注销\u0026#34;\u0026#34;\u0026#34; options = webdriver.ChromeOptions() options.binary_location = chrome_location options.add_argument(\u0026#34;ignore-certificate-errors\u0026#34;) options.add_argument(\u0026#34;--headless\u0026#34;) options.add_argument(\u0026#34;--disable-gpu\u0026#34;) options.add_argument(\u0026#34;--disable-software-rasterizer\u0026#34;) options.add_experimental_option(\u0026#34;detach\u0026#34;, True) driver = webdriver.Chrome(options=options,executable_path=\u0026#39;../chromedriver.exe\u0026#39;) login_url = \u0026#39;https://account.chsi.com.cn/passport/login?service=https://www.ncss.cn/student/connect/chsi\u0026amp;entrytype=stu\u0026#39; driver.get(login_url) time.sleep(1) acc_input = driver.find_element(By.XPATH, r\u0026#39;/html/body/div/div[2]/div[2]/div/div[2]/form/div[1]/input\u0026#39;) acc_input.send_keys(username) pwd_input = driver.find_element(By.XPATH, f\u0026#39;/html/body/div/div[2]/div[2]/div/div[2]/form/div[2]/input\u0026#39;) pwd_input.send_keys(password) driver.find_element(By.XPATH, r\u0026#39;/html/body/div/div[2]/div[2]/div/div[2]/form/div[4]\u0026#39;).click() element = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.XPATH, r\u0026#34;/html/body/div/span/span/a[1]\u0026#34;))) element.click() dic = {} for i in driver.get_cookies(): dic[i[\u0026#39;name\u0026#39;]] = i[\u0026#39;value\u0026#39;] return dic def Cookie_concat(got_property): \u0026#34;\u0026#34;\u0026#34;将从keep_connect函数中获取到的Cookie数据进行重新包装\u0026#34;\u0026#34;\u0026#34; changed_property_list = [\u0026#39;SESSION\u0026#39;, \u0026#39;Hm_lpvt_378ff17a1ac046691cf78376632d1ed4\u0026#39;, \u0026#39;_ga\u0026#39;,\u0026#39;_ga_6CXWRD3K0D\u0026#39;] no_change = { \u0026#39;_ga_1ESVLDHDYL\u0026#39;: \u0026#39;GS1.1.1726503021.3.0.1726503021.0.0.0\u0026#39;, \u0026#39;_abfpc\u0026#39;: \u0026#39;d7760a7e8d00953eaa18b111273b00d6ad83f1cc_2.0\u0026#39;, \u0026#39;cna\u0026#39;: \u0026#39;eac1e178d67b00529df2b287b5842f2c\u0026#39;, \u0026#39;_gid\u0026#39;: \u0026#39;GA1.2.673435621.1755336171\u0026#39;, \u0026#39;aliyungf_tc\u0026#39;: \u0026#39;bfc4565f794c9e6ac253a5c09f4a937f2afb69e01fc7b413d486aaeaadf06249\u0026#39;, \u0026#39;XSRF-CCKTOKEN\u0026#39;: \u0026#39;a12b3436b08549ba2f33a4aefa9a1454\u0026#39;, \u0026#39;CHSICC_CLIENTFLAGNCSS\u0026#39;: \u0026#39;aba5b818eaa8bc94d6fb1ddf17d1df4f\u0026#39;, \u0026#39;CHSICC01\u0026#39;: \u0026#39;!DzrVNB/pHD1H78bzYxYLahOzddj6Y4XQ6NJ5RnOPIOyzHzKixC+5X5WINIjztT+S4x5PGaf/cowaI/Q=\u0026#39;, \u0026#39;CHSICC_CLIENTFLAGSTUDENT\u0026#39;: \u0026#39;5d0ab9cce044f18a699886e7d6705555\u0026#39;, \u0026#39;Hm_lvt_378ff17a1ac046691cf78376632d1ed4\u0026#39;: \u0026#39;1754926580,1754968150,1755336169,1755411478\u0026#39;, \u0026#39;HMACCOUNT\u0026#39;: \u0026#39;CEB955474E107530\u0026#39;, \u0026#39;acw_tc\u0026#39;: \u0026#39;ac11000117554230396983911ee7b259a823321b52a20a878c0b42ee677273\u0026#39;, \u0026#39;_gat_gtag_UA_105074615_1\u0026#39;: \u0026#39;1\u0026#39; } property_all = no_change.copy() for i in changed_property_list: property_all[i] = got_property[i] property_rank = [\u0026#39;SESSION\u0026#39;,\u0026#39;_ga_1ESVLDHDYL\u0026#39;,\u0026#39;_abfpc\u0026#39;,\u0026#39;cna\u0026#39;,\u0026#39;_gid\u0026#39;,\u0026#39;aliyungf_tc\u0026#39;,\u0026#39;acw_tc\u0026#39;,\u0026#39;XSRF-CCKTOKEN\u0026#39;,\u0026#39;CHSICC_CLIENTFLAGNCSS\u0026#39;,\u0026#39;CHSICC01\u0026#39;,\u0026#39;CHSICC_CLIENTFLAGSTUDENT\u0026#39;,\u0026#39;Hm_lvt_378ff17a1ac046691cf78376632d1ed4\u0026#39;,\u0026#39;HMACCOUNT\u0026#39;,\u0026#39;_gat_gtag_UA_105074615_1\u0026#39;,\u0026#39;Hm_lpvt_378ff17a1ac046691cf78376632d1ed4\u0026#39;,\u0026#39;_ga\u0026#39;,\u0026#39;_ga_6CXWRD3K0D\u0026#39;] Cookie=\u0026#39;\u0026#39; for i in property_rank: Cookie = Cookie + i + \u0026#39;=\u0026#39; + property_all[i] + \u0026#39;; \u0026#39; print(\u0026#34;------------------完成Cookie创建------------------\u0026#34;) return Cookie def simulate_login_get_cookie(chrome_location,username,password): \u0026#34;\u0026#34;\u0026#34;将模拟登录部分和包装Cookie部分整合在一起\u0026#34;\u0026#34;\u0026#34; dic = keep_connect(chrome_location=chrome_location,username=username,password=password) return Cookie_concat(dic) ua = UserAgent() def generate_timestamp(): \u0026#34;\u0026#34;\u0026#34;生成13位时间戳\u0026#34;\u0026#34;\u0026#34; return str(int(time.time() * 1000)) def get_job_data(http,Cookie,page=1,page_size=10): \u0026#34;\u0026#34;\u0026#34;获取岗位数据\u0026#34;\u0026#34;\u0026#34; baseurl = \u0026#34;https://www.ncss.cn/student/jobs/jobslist/ajax/\u0026#34; #构造请求头 headers = { \u0026#39;User-Agent\u0026#39;:ua.random, \u0026#39;Connection\u0026#39;:\u0026#39;keep-alive\u0026#39;, \u0026#39;Cookie\u0026#39;:Cookie, \u0026#34;Accept\u0026#34;: \u0026#34;application/json,*/*\u0026#34;, \u0026#39;Referer\u0026#39;:\u0026#34;https://account.chsi.com.cn/passport/login?service=https://job.ncss.cn/student/connect/chsi\u0026amp;entrytype=stu\u0026#34; } # 查询参数 params = { \u0026#34;jobType\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;areaCode\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;jobName\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;monthPay\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;industrySectors\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;property\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;categoryCode\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;memberLevel\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;recruitType\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;offset\u0026#34;: page, \u0026#34;limit\u0026#34;: page_size, \u0026#34;keyUnits\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;degreeCode\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;sourcesName\u0026#34;: \u0026#34;0\u0026#34;, \u0026#34;sourcesType\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;_\u0026#34;: generate_timestamp() # 动态时间戳 } try: resp = http.request( method=\u0026#39;GET\u0026#39;, url=baseurl, fields=params, headers=headers ) if resp.status in [403,401]: tqdm.write(f\u0026#34;请求第{page}页时登录过期\u0026#34;) return None elif resp.status == 200: data = json.loads(resp.data.decode(\u0026#39;UTF-8\u0026#39;)) return data[\u0026#39;data\u0026#39;][\u0026#39;list\u0026#39;] else: tqdm.write(f\u0026#34;请求第{page}页时错误，状态码{resp.status}\u0026#34;) return None except Exception as e: tqdm.write(f\u0026#34;请求第{page}页时发生错误:{e}\u0026#34;) return None def fetch_detail_page(job_id,http,Cookie): \u0026#34;\u0026#34;\u0026#34;获取详情页数据\u0026#34;\u0026#34;\u0026#34; headers = { \u0026#39;User-Agent\u0026#39;:ua.random, \u0026#39;Cookie\u0026#39;:Cookie, \u0026#39;Referer\u0026#39;:\u0026#39;https://job.ncss.cn/student/jobs/index.html\u0026#39; } detail_url = f\u0026#34;https://www.ncss.cn/student/jobs/{job_id}/detail.html\u0026#34; try: resp = http.request( \u0026#39;GET\u0026#39;, detail_url, headers = headers ) if resp.status == 200: return resp.data.decode(\u0026#39;utf-8\u0026#39;) elif resp.status in [401,403]: tqdm.write(f\u0026#34;获取{job_id}详情页时登录过期\u0026#34;) return None else: tqdm.write(f\u0026#34;请求失败，状态码{resp.status}\u0026#34;) return None except Exception as e: tqdm.write(f\u0026#34;请求失败，{str(e)}\u0026#34;) return None def pares_detail_job_info(html,job_info): \u0026#34;\u0026#34;\u0026#34;将爬取到的详情页进行解析，获取“岗位介绍”部分的数据\u0026#34;\u0026#34;\u0026#34; soup = BeautifulSoup(html,\u0026#39;html.parser\u0026#39;) job_detail_describe_div =soup.find(name=\u0026#39;div\u0026#39;,attrs={\u0026#39;class\u0026#39;:\u0026#34;details\u0026#34;}) if job_detail_describe_div is not None: job_detail_describe = job_detail_describe_div.getText() else: tqdm.write(f\u0026#34;未找到岗位描述元素，岗位ID:{job_info.get(\u0026#39;岗位ID\u0026#39;)}\u0026#34;) job_detail_describe = \u0026#34;未知\u0026#34; job_info.update({ \u0026#34;岗位介绍\u0026#34;:job_detail_describe }) return job_info def parse_job_info(job): \u0026#34;\u0026#34;\u0026#34;解析岗位信息\u0026#34;\u0026#34;\u0026#34; id = job.get(\u0026#34;jobId\u0026#34;) timeStamp = job.get(\u0026#34;updateDate\u0026#34;) timeArray = time.localtime(float(timeStamp)/1000) return { \u0026#34;岗位ID\u0026#34;:id, \u0026#34;职位名称\u0026#34;:job.get(\u0026#34;jobName\u0026#34;), \u0026#34;薪资水平\u0026#34;:str(job.get(\u0026#34;lowMonthPay\u0026#34;))+\u0026#39;k-\u0026#39;+str(job.get(\u0026#39;highMonthPay\u0026#39;))+\u0026#39;k\u0026#39;, \u0026#34;招聘人数\u0026#34;:job.get(\u0026#34;headCount\u0026#34;), \u0026#34;学历要求\u0026#34;:job.get(\u0026#34;degreeName\u0026#34;), \u0026#34;招聘方\u0026#34;:job.get(\u0026#34;recName\u0026#34;), \u0026#34;公司规模\u0026#34;:job.get(\u0026#34;recScale\u0026#34;), \u0026#34;地区\u0026#34;:job.get(\u0026#34;areaCodeName\u0026#34;), \u0026#34;福利\u0026#34;:job.get(\u0026#34;recTags\u0026#34;), \u0026#34;专业要求\u0026#34;:job.get(\u0026#34;major\u0026#34;), \u0026#34;岗位更新时间\u0026#34;:time.strftime(\u0026#34;%Y-%m-%d %H:%M:%S\u0026#34;,timeArray), \u0026#34;详情网址\u0026#34;:f\u0026#34;https://www.ncss.cn/student/jobs/{id}/detail.html\u0026#34; } def list_page_worker(list_page_queue, http, Cookie, detail_task_queue, list_pbar): \u0026#34;\u0026#34;\u0026#34;获取岗位列表的工作流程\u0026#34;\u0026#34;\u0026#34; global list_get_wrong, list_pages_done, enqueued_jobs_count while True: page = None try: page = list_page_queue.get(timeout=30) if page is None: list_page_queue.task_done() # 对应主线程 put(None) break # 爬取列表页 jobs = get_job_data(http=http, Cookie=Cookie, page=page, page_size=10) if jobs: for job in jobs: job_info = parse_job_info(job) detail_task_queue.put(job_info) with progress_lock: enqueued_jobs_count += 1 time.sleep(random.uniform(0.5, 1.5)) except queue.Empty: thread_name = threading.current_thread().name tqdm.write(f\u0026#34;列表页任务队列空，线程 {thread_name} 准备退出\u0026#34;) continue except Exception as e: tqdm.write(f\u0026#34;列表页工作线程异常: {str(e)}\u0026#34;) if page: list_get_wrong.append(page) time.sleep(5) finally: if page is not None: # 只在真实任务时更新 list_pages_done += 1 list_pbar.update(1) list_page_queue.task_done() def detail_page_worker(detail_task_queue,result_queue,http,Cookie,detail_pbar): \u0026#34;\u0026#34;\u0026#34;详情页工作流程\u0026#34;\u0026#34;\u0026#34; global detail_get_wrong,detail_jobs_done while True: job_info = None job_id =None try: job_info = detail_task_queue.get(timeout=30) if job_info is None: detail_task_queue.task_done() break job_id = job_info.get(\u0026#34;岗位ID\u0026#34;) # print(f\u0026#34;开始爬取详情页{job_id}\u0026#34;) html = fetch_detail_page(job_id, http, Cookie) if html: new_job_info = pares_detail_job_info(html, job_info) result_queue.put(new_job_info) # print(f\u0026#34;详情页{job_id}解析完成\u0026#34;) else: tqdm.write(f\u0026#34;详情页{job_id}爬取失败\u0026#34;) time.sleep(random.uniform(0.3, 0.8)) except queue.Empty: thread_name = threading.current_thread().name tqdm.write(f\u0026#34;详情页任务队列空，线程{thread_name}准备退出\u0026#34;) continue except Exception as e: tqdm.write(f\u0026#34;详情页工作线程异常: {str(e)}\u0026#34;) if job_id: detail_get_wrong.append(job_id) time.sleep(3) finally: if job_info is not None: detail_jobs_done += 1 detail_pbar.update(1) detail_task_queue.task_done() def result_writer(result_queue, output_address): \u0026#34;\u0026#34;\u0026#34;结果写入线程\u0026#34;\u0026#34;\u0026#34; struc_time = time.localtime() time_year = struc_time.tm_year time_month = struc_time.tm_mon time_day = struc_time.tm_mday output_file = output_address + f\u0026#39;{time_year}_{time_month}_{time_day}_jobs\u0026#39; count = 0 with open(output_file, \u0026#34;a\u0026#34;, encoding=\u0026#39;utf-8\u0026#39;, newline=\u0026#39;\u0026#39;) as f: while True: try: # 获取结果 result = result_queue.get(timeout=120) if result is None: break f.write(json.dumps(result, ensure_ascii=False) + \u0026#34;\\n\u0026#34;) f.flush() count += 1 if count % 100 == 0: tqdm.write(f\u0026#34;已写入 {count} 条结果\u0026#34;) result_queue.task_done() except queue.Empty: tqdm.write(\u0026#34;结果写入线程超时退出\u0026#34;) break except Exception as e: tqdm.write(f\u0026#34;结果写入异常: {str(e)}\u0026#34;) tqdm.write(f\u0026#34;结果写入完成，共写入 {count} 条数据\u0026#34;) def process_manager(total_pages,http,Cookie,num_list_workers=2,num_detail_workers=8,output_address=\u0026#34;\u0026#34;): \u0026#34;\u0026#34;\u0026#34;进程管理器（主进程）\u0026#34;\u0026#34;\u0026#34; global enqueued_jobs_count start_time = time.time() tqdm.write(f\u0026#34;开始爬取任务，总页数: {total_pages}\u0026#34;) #创建Manager with (Manager() as manager): #创建队列 list_page_queue = manager.Queue() detail_task_queue = manager.Queue(maxsize=5000) result_queue = manager.Queue() #添加列表页任务 for page in range(1,total_pages+1): list_page_queue.put(page) expected_total_list_pages = total_pages expected_total_detail_jobs = total_pages * 10 #进度条 list_pbar = tqdm(total=expected_total_list_pages,desc=\u0026#39;列表页\u0026#39;,unit=\u0026#39;页\u0026#39;,dynamic_ncols=True,file=sys.stdout) detail_pbar = tqdm(total=expected_total_detail_jobs,desc=\u0026#39;详情页\u0026#39;,unit=\u0026#39;项\u0026#39;,dynamic_ncols=True,file=sys.stdout) #创建并启动线程 threads = [] #列表页工作线程 for i in range(num_list_workers): t = threading.Thread( target=list_page_worker, args=(list_page_queue,http,Cookie,detail_task_queue,list_pbar), name=f\u0026#34;ListWorker-{i+1}\u0026#34;, daemon=True ) t.start() threads.append(t) #详情页工作线程 for i in range(num_detail_workers): t = threading.Thread( target=detail_page_worker, args=(detail_task_queue,result_queue,http,Cookie,detail_pbar), name=f\u0026#34;DetailWorker-{i+1}\u0026#34;, daemon=True ) t.start() threads.append(t) #结果写入线程 writer_thread = threading.Thread( target=result_writer, args=(result_queue,output_address), name=\u0026#34;ResultWriter\u0026#34;, daemon=True ) writer_thread.start() #等待列表页队列为空（等待完成列表页任务） list_page_queue.join() #通知列表页任务线程退出 for _ in range(num_list_workers): list_page_queue.put(None) tqdm.write(f\u0026#34;所有列表页任务已完成,{num_list_workers}个工作线程已退出\u0026#34;) #调整详情任务总数 with progress_lock: actual_enqueued = enqueued_jobs_count if actual_enqueued \u0026gt; 0 and actual_enqueued \u0026lt; detail_pbar.total: detail_pbar.total = actual_enqueued detail_pbar.refresh() #等待详情页任务队列为空（详情页任务完成） detail_task_queue.join() #通知详情页工作线程退出 for _ in range(num_detail_workers): detail_task_queue.put(None) tqdm.write(f\u0026#34;所有详情页任务已完成,{num_detail_workers}个工作线程已退出\u0026#34;) #通知结果写入线程退出 result_queue.put(None) for t in threads: t.join(timeout=10) writer_thread.join(timeout=10) elapsed_time = time.time() - start_time list_pbar.close() detail_pbar.close() print(f\u0026#34;爬取完成! 总耗时: {elapsed_time:.2f} 秒\u0026#34;) if __name__ == \u0026#34;__main__\u0026#34;: #全局变量 progress_lock = threading.Lock() list_pages_done = 0 detail_jobs_done =0 enqueued_jobs_count = 0 # 配置爬取参数 TOTAL_PAGES = 200 # 要爬取的列表页总数 NUM_LIST_WORKERS = 2 # 列表页工作线程数 NUM_DETAIL_WORKERS = 5 # 详情页工作线程数 OUTPUT_ADDRESS = \u0026#34;\u0026#34; # 输出文件的地址，默认为项目地址 #配置模拟登录参数 chrome_location = r\u0026#39;D:\\python_project\\Google\\Chrome\\Application\\chrome.exe\u0026#39; #chrome启动器的位置 username = \u0026#34;15897310548\u0026#34; #模拟登录的账号 password = \u0026#34;2453829998Hu\u0026#34;#模拟登陆的密码 detail_get_wrong = [] #获取详情失败的job_id list_get_wrong = [] #获取岗位失败的页码 #构建连接池 print(\u0026#34;------------------开始构建连接池------------------\u0026#34;) http = urllib3.PoolManager( num_pools=50, maxsize=50, cert_reqs=\u0026#39;CERT_NONE\u0026#39;, assert_hostname=False, block=True, timeout=urllib3.Timeout(connect=5.0, read=10.0), retries=urllib3.Retry(total=3, backoff_factor=0.5), ) print(\u0026#34;------------------完成构建连接池------------------\u0026#34;) #模拟登录，维持登录状态，获取组成Cookie的必要的参数 print(\u0026#34;-------------------开始模拟登录-------------------\u0026#34;) Cookie = simulate_login_get_cookie(chrome_location=chrome_location,username=username,password=password) print(\u0026#34;-------------------完成模拟登录-------------------\u0026#34;) # 启动爬虫 print(\u0026#34;-------------------开始网络爬取-------------------\u0026#34;) process_manager( total_pages=TOTAL_PAGES, num_list_workers=NUM_LIST_WORKERS, num_detail_workers=NUM_DETAIL_WORKERS, output_address=OUTPUT_ADDRESS, http = http, Cookie = Cookie ) print(\u0026#34;-------------------完成网络爬取-------------------\u0026#34;) 问题 OR 改进 当前的爬取仍需要登录才能获取，如何才能以非登录状态获取所需的数据 可以增加代理，从而不断变动IP使得不易被锁IP ","date":"2025-08-20T17:47:51+08:00","image":"https://Entars.github.io/p/%E5%9B%BD%E5%AE%B6%E5%A4%A7%E5%AD%A6%E7%94%9F%E5%B0%B1%E4%B8%9A%E6%9C%8D%E5%8A%A1%E5%B9%B3%E5%8F%B0%E5%B2%97%E4%BD%8D%E7%88%AC%E5%8F%96/national_college_employment_platform_hu_a3782813dccbc16d.png","permalink":"https://Entars.github.io/p/%E5%9B%BD%E5%AE%B6%E5%A4%A7%E5%AD%A6%E7%94%9F%E5%B0%B1%E4%B8%9A%E6%9C%8D%E5%8A%A1%E5%B9%B3%E5%8F%B0%E5%B2%97%E4%BD%8D%E7%88%AC%E5%8F%96/","title":"国家大学生就业服务平台岗位爬取"},{"content":"BeautifulSoup 简介 BeautifulSoup 是一个可以把 HTML 或 XML 文档“变成结构化树”，方便你用Python 来查找、遍历、提取内容的工具。\n解析器 解析器 使用方法 优势 劣势 Python标准库 BeautifulSoup(markup, \u0026lsquo;html.parser\u0026rsquo;) python内置的标准库，执行速度适中 Python3.2.2之前的版本容错能力差 lxml HTML解析器 BeautifulSoup(markup, \u0026rsquo;lxml') 速度快、文档容错能力强 需要安装C语言库 lxml XML解析器 BeautifulSoup(markup \u0026lsquo;xml\u0026rsquo;) 速度快，唯一支持XML的解析器 需要安装C语言库 html5lib BeautifulSoup(markup, \u0026lsquo;html5lib\u0026rsquo;) 最好的容错性、以浏览器的方式解析文档、生成HTML5格式的文档 速度慢，不依赖外部拓展 具体使用代码展示如下：\n1 2 3 4 from bs4 import BeautifulSoup soup = BeautifulSoup(\u0026#39;\u0026lt;p\u0026gt;Hello world\u0026lt;/p\u0026gt;\u0026#39;, \u0026#39;lxml\u0026#39;) print(soup.p) 基本用法 当传入不标准的HTML字符串，BeautifulSoup可以自动更正格式。\nsoup.prettify(): 用于将 HTML 或 XML 文档格式化输出为带缩进的字符串,即会把原始网页内容重新排版，输出一个带有层级缩进的漂亮 HTML 字符串，例：\n1 2 3 4 5 6 7 from bs4 import BeautifulSoup html = \u0026#39;\u0026lt;html\u0026gt;\u0026lt;head\u0026gt;\u0026lt;title\u0026gt;Test\u0026lt;/title\u0026gt;\u0026lt;/head\u0026gt;\u0026lt;body\u0026gt;\u0026lt;h1\u0026gt;Hi\u0026lt;/h1\u0026gt;\u0026lt;p\u0026gt;Hello\u0026lt;/p\u0026gt;\u0026lt;/body\u0026gt;\u0026lt;/html\u0026gt;\u0026#39; soup = BeautifulSoup(html, \u0026#39;html.parser\u0026#39;) pretty_html = soup.prettify() print(pretty_html) 输出：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt; Test \u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt; Hi \u0026lt;/h1\u0026gt; \u0026lt;p\u0026gt; Hello \u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; .title：获取title标签\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 html_doc=\u0026#34;\u0026#34;\u0026#34; \u0026lt;html\u0026gt;\u0026lt;head\u0026gt;\u0026lt;title\u0026gt;The Dormouse\u0026#39;s story\u0026lt;/title\u0026gt;\u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p class=\u0026#34;title\u0026#34;\u0026gt;\u0026lt;b\u0026gt;The Dormouse\u0026#39;s story\u0026lt;/b\u0026gt;\u0026lt;/p\u0026gt; \u0026lt;p class=\u0026#34;story\u0026#34;\u0026gt;Once upon a time there were three little sisters; and their names were \u0026lt;a href=\u0026#34;http://example.com/elsie\u0026#34; class=\u0026#34;sister\u0026#34; id=\u0026#34;link1\u0026#34;\u0026gt;Elsie\u0026lt;/a\u0026gt;, \u0026lt;a href=\u0026#34;http://example.com/lacie\u0026#34; class=\u0026#34;sister\u0026#34; id=\u0026#34;link2\u0026#34;\u0026gt;Lacie\u0026lt;/a\u0026gt; and \u0026lt;a href=\u0026#34;http://example.com/tillie\u0026#34; class=\u0026#34;sister\u0026#34; id=\u0026#34;link3\u0026#34;\u0026gt;Tillie\u0026lt;/a\u0026gt;; and they lived at the bottom of a well.\u0026lt;/p\u0026gt; \u0026lt;p class=\u0026#34;story\u0026#34;\u0026gt;...\u0026lt;/p\u0026gt; \u0026#34;\u0026#34;\u0026#34;\u0026#34; # 创建beautifulsoup对象 解析器为lxml soup = BeautifulSoup(html_doc, \u0026#39;lxml\u0026#39;) print(soup.title) #output-\u0026gt;\u0026lt;title\u0026gt;The Dormouse\u0026#39;s story\u0026lt;/title\u0026gt; .name：返回的是当前节点的“标签名称”\n1 2 3 4 5 soup = BeautifulSoup(html_doc, \u0026#39;lxml\u0026#39;) print(soup.title.name) print(soup.name) #output-\u0026gt;title #[document] .string/.text：获取标签中的文字内容\n1 2 3 4 5 soup = BeautifulSoup(html_doc, \u0026#39;lxml\u0026#39;) print(soup.title.string) print(soup.title.text) #output-\u0026gt;The Dormouse\u0026#39;s story #The Dormouse\u0026#39;s story .p：访问HTML中第一个标签\n1 2 3 soup = BeautifulSoup(html_doc, \u0026#39;lxml\u0026#39;) print(soup.p) #output-\u0026gt;\u0026lt;p class=\u0026#34;title\u0026#34;\u0026gt;\u0026lt;b\u0026gt;The Dormouse\u0026#39;s story\u0026lt;/b\u0026gt;\u0026lt;/p\u0026gt; .find_all()：查找文档中所有符合条件的标签元素，返回一个列表\n用法 示例 说明 查找所有某种标签 soup.find_all(\u0026lsquo;p\u0026rsquo;) 找出所有 \u0026lt;p\u0026gt; 根据 class 属性 soup.find_all(\u0026lsquo;p\u0026rsquo;, class_=\u0026lsquo;story\u0026rsquo;) class 要用 class_ 表示 查找多个标签 soup.find_all([\u0026lsquo;p\u0026rsquo;, \u0026lsquo;a\u0026rsquo;]) 所有 \u0026lt;p\u0026gt; 和 \u0026lt;a\u0026gt; 标签 查找包含特定字符串的标签 soup.find_all(string=\u0026lsquo;Hello\u0026rsquo;) 内容匹配为 \u0026lsquo;Hello\u0026rsquo; 的标签 使用属性字典 soup.find_all(\u0026lsquo;a\u0026rsquo;, {\u0026lsquo;id\u0026rsquo;: \u0026rsquo;link1\u0026rsquo;}) 属性筛选 限制返回数量 soup.find_all(\u0026lsquo;p\u0026rsquo;, limit=2) 最多返回 2 个 \u0026lt;p\u0026gt; 标签 1 2 3 4 \u0026#39;\u0026#39;\u0026#39; 基本语法： soup.find_all(name=None, attrs={}, recursive=True, string=None, limit=None, **kwargs) \u0026#39;\u0026#39;\u0026#39; .find()：获取第一次匹配条件的元素\n1 2 3 soup = BeautifulSoup(html_doc, \u0026#39;lxml\u0026#39;) print(soup.find(id=\u0026#34;link1\u0026#34;)) #output-\u0026gt;\u0026lt;a class=\u0026#34;sister\u0026#34; href=\u0026#34;https://example.com/elsie\u0026#34; id=\u0026#34;link1\u0026#34;\u0026gt;Elsie\u0026lt;/a\u0026gt; .parent：获取父级标签\n1 2 3 4 5 soup = BeautifulSoup(html_doc, \u0026#39;lxml\u0026#39;) print(soup.title) print(soup.title.parnt) #output-\u0026gt;\u0026lt;title\u0026gt;The Dormouse\u0026#39;s story\u0026lt;/title\u0026gt; #\u0026lt;head\u0026gt;\u0026lt;title\u0026gt;The Dormouse\u0026#39;s story\u0026lt;/title\u0026gt;\u0026lt;/head\u0026gt; .p[\u0026lsquo;class\u0026rsquo;]：获取某个 \u0026lt;p\u0026gt; 标签的 class 属性的值\n1 2 3 soup = BeautifulSoup(html_doc, \u0026#39;lxml\u0026#39;) print(soup.p[\u0026#34;class\u0026#34;]) #output-\u0026gt;[\u0026#39;title\u0026#39;] .get_text()：获取文档中所有文字内容\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 soup = BeautifulSoup(html_doc, \u0026#39;lxml\u0026#39;) print(soup.get_text()) \u0026#39;\u0026#39;\u0026#39; output-\u0026gt; The Dormouse\u0026#39;s story The Dormouse\u0026#39;s story Once upon a time there were three little sisters; and their names were Elsie, Lacie and Tillie; and they lived at the bottom of a well. ... \u0026#39;\u0026#39;\u0026#39; .get()：用于安全地获取某个属性的值\n1 2 3 4 5 6 7 8 9 \u0026#39;\u0026#39;\u0026#39; tag.get(\u0026#39;属性名\u0026#39;) 相当于 tag[\u0026#39;属性名\u0026#39;]，但如果属性不存在，不会报错，而是返回 None \u0026#39;\u0026#39;\u0026#39; a_tags = soup.find_all(\u0026#39;a\u0026#39;) for a_tag in a_tags: print(a_tag.get(\u0026#34;href\u0026#34;)) #output-\u0026gt;https://example.com/elsie #https://example.com/lacie #https://example.com/tillie BeautifulSoup的对象种类 主要有四种主要类型：Tag，NavigableString，BeautifulSoup，Comment。\nTag：该对象与HTML或XML原生文档中的标签相同，该对象可以包含其他标签，文本内容和属性。\n1 2 3 4 soup = BeautifulSoup(html_doc, \u0026#39;lxml\u0026#39;) tag = soup.title print(type(tag)) #output-\u0026gt;\u0026lt;class \u0026#39;bs4.element.Tag\u0026#39;\u0026gt; NavigableString：标签中的文本内容，是一个不可变字符串，可以由Tag对象的.string获取\n1 2 3 4 soup = BeautifulSoup(html_doc, \u0026#39;lxml\u0026#39;) tag = soup.title print(type(tag.string)) #output-\u0026gt; \u0026lt;class \u0026#39;bs4.element.NavigableString\u0026#39;\u0026gt; BeautifulSoup：整个文档的根对象，即整个文档的根内容，可以被视为一个特殊的Tag对象，但没有名称和属性，其提供对整个文档的遍历，搜索和修改\n1 2 3 soup = BeautifulSoup(html_doc, \u0026#39;lxml\u0026#39;) print(type(soup)) #output-\u0026gt; \u0026lt;class \u0026#39;bs4.BeautifulSoup\u0026#39;\u0026gt; Comment：对象是一个特殊类型的NavigableString对象,表示HTML和XML中的注释部分\n1 2 3 4 # \u0026lt;b\u0026gt;\u0026lt;!--This is a comment--\u0026gt;\u0026lt;/b\u0026gt; soup = BeautifulSoup(html_doc, \u0026#39;lxml\u0026#39;) print(type(soup.b.string)) #output-\u0026gt; \u0026lt;class \u0026#39;bs4.element.NavigableString\u0026#39;\u0026gt; BeautifulSoup遍历文档树 BeautifulSoup提供很多方法来遍历解析后的文档树\n导航父节点：.parent和.parents。其中.parent可以获取当前节点的上一级父节点，.parents可以遍历获取当前节点的所有父辈节点\n1 2 3 4 soup = BeautifulSoup(html_doc, \u0026#39;lxml\u0026#39;) title_tag = soup.title print(title_tag.parent) #\u0026lt;head\u0026gt;\u0026lt;title\u0026gt;The Dormouse\u0026#39;s story\u0026lt;/title\u0026gt;\u0026lt;/head\u0026gt; 1 2 3 4 5 6 7 8 9 10 soup = BeautifulSoup(html_doc, \u0026#39;lxml\u0026#39;) body_tag = soup.body for parent in body_tag.parents: print(parent) #\u0026lt;html\u0026gt;\u0026lt;head\u0026gt;\u0026lt;title\u0026gt;The Dormouse\u0026#39;s story\u0026lt;/title\u0026gt;\u0026lt;/head\u0026gt; #\u0026lt;body\u0026gt; #\u0026lt;p class=\u0026#34;title\u0026#34;\u0026gt;\u0026lt;b\u0026gt;The Dormouse\u0026#39;s story\u0026lt;/b\u0026gt;\u0026lt;/p\u0026gt; #\u0026lt;p class=\u0026#34;story\u0026#34;\u0026gt;Once upon a time there were three little sisters; and their names were #\u0026lt;a class=\u0026#34;sister\u0026#34; href=\u0026#34;https://example.com/elsie\u0026#34; id=\u0026#34;link1\u0026#34;\u0026gt;Elsie\u0026lt;/a\u0026gt;, #.... 导航子节点：.contents可以获取当前节点的所有子节点；.children可以遍历当前节点的所有子节点，返回一个list。字符串没有这两个属性。这两个仅包含tag直接子结点，字符串”The Dormouse\u0026rsquo;s story”是\u0026lt;head\u0026gt;标签的子孙结点\n1 2 3 4 soup = BeautifulSoup(html_doc, \u0026#39;lxml\u0026#39;) head_contents = soup.head.contents print(head_contents) #output-\u0026gt; [\u0026lt;title\u0026gt;The Dormouse\u0026#39;s story\u0026lt;/title\u0026gt;] 1 2 3 4 5 6 7 8 9 10 soup = BeautifulSoup(html_doc, \u0026#39;lxml\u0026#39;) body_children = soup.body.children for child in body_children: print(child) #output-\u0026gt;\u0026lt;p class=\u0026#34;title\u0026#34;\u0026gt;\u0026lt;b\u0026gt;The Dormouse\u0026#39;s story\u0026lt;/b\u0026gt;\u0026lt;/p\u0026gt; #\u0026lt;a class=\u0026#34;sister\u0026#34; href=\u0026#34;https://example.com/elsie\u0026#34; id=\u0026#34;link1\u0026#34;\u0026gt;Elsie\u0026lt;/a\u0026gt;, #\u0026lt;a class=\u0026#34;sister\u0026#34; href=\u0026#34;https://example.com/tillie\u0026#34; id=\u0026#34;link3\u0026#34;\u0026gt;Tillie\u0026lt;/a\u0026gt;; #and they lived at the bottom of a well.\u0026lt;/p\u0026gt; #..... .descendants：可以遍历当前节点的所有后代节点（层遍历）\n1 2 3 soup = BeautifulSoup(html_doc, \u0026#39;lxml\u0026#39;) for descendant in soup.descendants: print(descendant) 节点内容：.string，.strings，.stripped_strings。.string如果如果tag只有一个NavigableString类型子节点,那么这个tag可以使用.string得到其子节点。但若tag中包含了多个子节点,tag就无法确定string方法应该调用哪一个字节的内容,则会输出None\n1 2 3 4 5 soup = BeautifulSoup(html_doc, \u0026#39;lxml\u0026#39;) print(soup.head.string) #The Dormouse\u0026#39;s story print(soup.title.string) #The Dormouse\u0026#39;s story 1 2 3 soup = BeautifulSoup(html_doc, \u0026#39;lxml\u0026#39;) print(soup.body.string) #None .strings可以遍历获取标签中的所有文本内容，.stripped_strings可以去除多余的空白字符\n1 2 3 4 5 6 7 soup = BeautifulSoup(html_doc, \u0026#39;lxml\u0026#39;) for string in soup.strings: print(string) #The Dormouse\u0026#39;s story ...... #The Dormouse\u0026#39;s story BeautifulSoup搜索文档树 find_all()：搜索当前tag的所有tag子节点\n1 2 3 4 5 6 7 8 9 10 11 soup = BeautifulSoup(html_doc, \u0026#39;lxml\u0026#39;) print(soup.find_all(\u0026#34;title\u0026#34;)) # 查找所有的title标签 print(soup.find_all(\u0026#34;p\u0026#34;, \u0026#34;title\u0026#34;)) # 查找p标签中class为title的标签 print(soup.find_all(\u0026#34;a\u0026#34;)) # 查找所有的a标签 print(soup.find_all(id=\u0026#34;link2\u0026#34;)) # 查找id为link2的标签 #[\u0026lt;title\u0026gt;The Dormouse\u0026#39;s story\u0026lt;/title\u0026gt;] #[\u0026lt;p class=\u0026#34;title\u0026#34;\u0026gt;\u0026lt;b\u0026gt;The Dormouse\u0026#39;s story\u0026lt;/b\u0026gt;\u0026lt;/p\u0026gt;] #[\u0026lt;a class=\u0026#34;sister\u0026#34; href=\u0026#34;https://example.com/elsie\u0026#34; id=\u0026#34;link1\u0026#34;\u0026gt;Elsie\u0026lt;/a\u0026gt;, \u0026lt;a class=\u0026#34;sister\u0026#34; href=\u0026#34;https://example.com/lacie\u0026#34; id=\u0026#34;link2\u0026#34;\u0026gt;Lacie\u0026lt;/a\u0026gt;, \u0026lt;a class=\u0026#34;sister\u0026#34; href=\u0026#34;https://example.com/tillie\u0026#34; id=\u0026#34;link3\u0026#34;\u0026gt;Tillie\u0026lt;/a\u0026gt;] #[\u0026lt;a class=\u0026#34;sister\u0026#34; href=\u0026#34;https://example.com/lacie\u0026#34; id=\u0026#34;link2\u0026#34;\u0026gt;Lacie\u0026lt;/a\u0026gt;] find_parent()：只返回最接近的父标签\n1 2 3 4 soup = BeautifulSoup(html_doc, \u0026#39;lxml\u0026#39;) a_string = soup.find(string=\u0026#39;Lacie\u0026#39;) print(a_string.find_parent()) # 查找父节点 #\u0026lt;a class=\u0026#34;sister\u0026#34; href=\u0026#34;https://example.com/lacie\u0026#34; id=\u0026#34;link2\u0026#34;\u0026gt;Lacie\u0026lt;/a\u0026gt; find_parents()：返回所有符合条件的祖先标签，按从近到远的顺序排列\n1 2 3 4 5 6 7 soup = BeautifulSoup(html_doc, \u0026#39;lxml\u0026#39;) a_string = soup.find(string=\u0026#39;Lacie\u0026#39;) print(a_string.find_parents()) # 查找父节点 #[\u0026lt;a class=\u0026#34;sister\u0026#34; href=\u0026#34;https://example.com/lacie\u0026#34; id=\u0026#34;link2\u0026#34;\u0026gt;Lacie\u0026lt;/a\u0026gt;, \u0026lt;p class=\u0026#34;story\u0026#34;\u0026gt;Once upon a time there were three little sisters; and their names were #\u0026lt;a class=\u0026#34;sister\u0026#34; href=\u0026#34;https://example.com/elsie\u0026#34; id=\u0026#34;link1\u0026#34;\u0026gt;Elsie\u0026lt;/a\u0026gt;, #\u0026lt;a class=\u0026#34;sister\u0026#34; href=\u0026#34;https://example.com/lacie\u0026#34; id=\u0026#34;link2\u0026#34;\u0026gt;Lacie\u0026lt;/a\u0026gt; and #and they lived at the bottom of a well.\u0026lt;/p\u0026gt;, \u0026lt;body\u0026gt;....] BeautifulSoup的CSS选择器 我们在写CSS时,标签名不加任何修饰,类名前加点,id名前加#,BeautifulSoup中也可以使用类似的方法来筛选元素。BeautifulSoup中的select()方法允许使用CSS选择器来查找HTML文档元素,其返回一个包含所有匹配元素的列表类似于find_all()方法。\n通过标签名查找：\n1 2 3 soup = BeautifulSoup(html_doc, \u0026#39;lxml\u0026#39;) print(soup.select(\u0026#39;b\u0026#39;)) #[\u0026lt;b\u0026gt;The Dormouse\u0026#39;s story\u0026lt;/b\u0026gt;, \u0026lt;b\u0026gt;\u0026lt;!--This is a comment--\u0026gt;\u0026lt;/b\u0026gt;] 通过类名查找：\n1 2 3 soup = BeautifulSoup(html_doc, \u0026#39;lxml\u0026#39;) print(soup.select(\u0026#39;.title\u0026#39;)) #[\u0026lt;p class=\u0026#34;title\u0026#34;\u0026gt;\u0026lt;b\u0026gt;The Dormouse\u0026#39;s story\u0026lt;/b\u0026gt;\u0026lt;/p\u0026gt;] id名查找：\n1 2 3 soup = BeautifulSoup(html_doc, \u0026#39;lxml\u0026#39;) print(soup.select(\u0026#39;#link1\u0026#39;)) #[\u0026lt;a class=\u0026#34;sister\u0026#34; href=\u0026#34;https://example.com/elsie\u0026#34; id=\u0026#34;link1\u0026#34;\u0026gt;Elsie\u0026lt;/a\u0026gt;] 组合查找：组合查找即与写class时一致,标签名与类名id名进行组合的原理一样\n1 2 3 soup = BeautifulSoup(html_doc, \u0026#39;lxml\u0026#39;) print(soup.select(\u0026#39;p #link1\u0026#39;)) #[\u0026lt;a class=\u0026#34;sister\u0026#34; href=\u0026#34;https://example.com/elsie\u0026#34; id=\u0026#34;link1\u0026#34;\u0026gt;Elsie\u0026lt;/a\u0026gt;] 属性查找：选择具有特定属性或属性值的标签\n简单属性选择器：\n1 2 3 soup = BeautifulSoup(html_doc, \u0026#39;lxml\u0026#39;) print(soup.select(\u0026#34;a[href=\u0026#39;https://example.com/elsie\u0026#39;]\u0026#34;)) # 选择a标签中href属性为https://example.com/elsie的标签 #[\u0026lt;a class=\u0026#34;sister\u0026#34; href=\u0026#34;https://example.com/elsie\u0026#34; id=\u0026#34;link1\u0026#34;\u0026gt;Elsie\u0026lt;/a\u0026gt;] 属性值选择器\n1 2 3 4 5 6 7 8 9 10 11 12 13 \u0026#39;\u0026#39;\u0026#39; 精确匹配:[attribute=\u0026#34;value\u0026#34;] 部分匹配 包含特定值:[attribute~=\u0026#34;value\u0026#34;] 选择属性值包含特定单词的标签。 以特定值开头:[attribute^=\u0026#34;value\u0026#34;] 选择属性值以特定字符串开头的标签 以特定值结尾:[attribute$=\u0026#34;value\u0026#34;] 选择属性值以特定字符串结尾的标签。 包含特定子字符串:[attribute*=\u0026#34;value\u0026#34;] 选择属性值包含特定子字符串的标签 \u0026#39;\u0026#39;\u0026#39; soup = BeautifulSoup(html_doc, \u0026#39;lxml\u0026#39;) print(soup.select(\u0026#39;a[href^=\u0026#34;https://example.com\u0026#34;]\u0026#39;)) # 选择href以https://example.com开头的a标签 #[\u0026lt;a class=\u0026#34;sister\u0026#34; href=\u0026#34;https://example.com/elsie\u0026#34; id=\u0026#34;link1\u0026#34;\u0026gt;Elsie\u0026lt;/a\u0026gt;, \u0026lt;a class=\u0026#34;sister\u0026#34; href=\u0026#34;https://example.com/lacie\u0026#34; id=\u0026#34;link2\u0026#34;\u0026gt;Lacie\u0026lt;/a\u0026gt;, \u0026lt;a class=\u0026#34;sister\u0026#34; href=\u0026#34;https://example.com/tillie\u0026#34; id=\u0026#34;link3\u0026#34;\u0026gt;Tillie\u0026lt;/a\u0026gt;] ","date":"2025-08-05T15:27:58+08:00","image":"https://Entars.github.io/p/beautifulsoup/BeautifulSoup_hu_e96c9b8184c5ae44.png","permalink":"https://Entars.github.io/p/beautifulsoup/","title":"BeautifulSoup"},{"content":"Urllib3 特点： 连接池管理：在客户端和服务器之间复用已有连接，避免每次请求都重新建立新连接，核心是PoolManager,内部维护一个或多个ConnectionPool； 线程安全：适合在多线程环境下进行并发请求； 重试机制：请求失败自动重试； SSL/TLS验证：建立 HTTPS 连接时，客户端校验服务器提供的数字证书是否可信，并通过 TLS 协议完成数据加密通道的协商； 代理支持：在客户端（你）访问目标网站时，通过一个中间服务器（代理服务器）中转请求和响应，而不是直接访问目标网站； 文件上传：支持 multipart 文件上传； 编码处理：自动处理响应内容的编码问题； 核心类与方法： PoolManager：最核心类，负责管理连接池和所有请求。\n1 2 3 4 5 6 7 8 http = urllib3.PoolManager( num_pools = 50, #连接池数量 maxsize = 10, #每个连接池最大连接数 block = True, #连接池满时是否阻塞等待 timeout = 30.0,\t#请求超时时间 retries = 3, #默认重试次数 headers={\u0026#39;User-Agent\u0026#39;:\u0026#39;\u0026#39;} ) GET请求：\n1 2 3 4 5 response = http.request( \u0026#39;GET\u0026#39;, \u0026#39;http://\u0026#39;, fields = {\u0026#39;arg\u0026#39;:\u0026#39;value\u0026#39;} #查询参数 ) POST请求：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 #表单数据 response = http.request( \u0026#39;POST\u0026#39;, \u0026#39;http://\u0026#39;, fields = {\u0026#39;field\u0026#39;:\u0026#39;value\u0026#39;} ) #JSON数据 import json response = http.request( \u0026#39;POST\u0026#39;, \u0026#39;http://\u0026#39;, body = json.dumps({\u0026#39;key\u0026#39;:\u0026#39;value\u0026#39;}).encode(\u0026#39;utf-8\u0026#39;), headers = {\u0026#39;Content-Type\u0026#39;:\u0026#39;application/json\u0026#39;} ) PUT/DELETE请求：\n1 2 3 4 5 6 7 8 9 10 11 12 #PUT请求 response = http.request( \u0026#39;PUT\u0026#39;, \u0026#39;http://\u0026#39;, body = b \u0026#39;data to put\u0026#39; ) #DELETE请求 response = http.request( \u0026#39;DELETE\u0026#39;, \u0026#39;http://\u0026#39; ) 文件上传：\n1 2 3 4 5 6 7 8 9 10 11 with open(\u0026#39;example.txt\u0026#39;,\u0026#39;rb\u0026#39;) as f: file_data = f.read() response = http.request( \u0026#39;POST\u0026#39;, \u0026#39;http://\u0026#39;, fields = { \u0026#39;filefield\u0026#39;:(\u0026#39;example.txt\u0026#39;,file_data,\u0026#39;text/plain\u0026#39;), \u0026#39;description\u0026#39;:\u0026#39;File upload example\u0026#39; } ) 响应处理与重要属性 响应对象属性：\n1 2 3 4 5 6 7 8 9 10 11 12 13 response = http.request(\u0026#39;GET\u0026#39;,\u0026#39;http://exmaple.com\u0026#39;) #状态码 print(response.status) #响应头 print(reponse.headers) #响应体 print(response.data) print(response.data.decode(\u0026#39;utf-8\u0026#39;))#解码为字符串 #重新定向历史 print(response.redirect_location) #消耗时间 print(response.elapsed) 响应内容处理：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 #JSON响应处理 import json json_response = json.loads(response.data.decode(\u0026#39;utf-8\u0026#39;)) #流式响应处理 response = http.request( \u0026#39;GET\u0026#39;, \u0026#39;http://exmaple.com/largefile\u0026#39;, preload_content=False ) try: for chunk in response.stream(1024):#每次读1024字节 process_chunk(chunk) finally: response.release_conn()#释放连接 高级特性与配置 重试机制：\n1 2 3 4 5 6 7 8 from urllib3.util.retry import Retry retry_strategy = Retry( total = 3, #重试总次数 backoff_factor = 1, #重试间隔增长因子 status_forcelist = [500,502,503,504]#指定哪些状态码会触发自动重试 ) http = urllib3.PoolManager(retries = retry_strategy) 超时设置：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 #全局超时 http = urllib3.PoolManager(timeout=2.0) #单个请求超时 response = http.request( \u0026#39;GET\u0026#39;, \u0026#39;http://example.com\u0026#39;, timeout=5.0 ) #分别设置连接和读取超时 response = http.request( \u0026#39;GET\u0026#39;, \u0026#39;http://example.com\u0026#39;, timeout=urllib3.Timeout(connect=2.0, read=10.0) ) SSL/TLS配置：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 #禁用证书验证(不推荐生产环境使用) http = urllib3.PoolManager( cert_reqs=\u0026#39;CERT_NONE\u0026#39;,#不校验证书有效性 assert_hostname=False #不检查证书是否匹配域名 ) #自定义CA证书 http = urllib3.PoolManager( cert_reqs=\u0026#39;CERT_REQUIRED\u0026#39;, ca_certs=\u0026#39;/path/to/certificate.pem\u0026#39;)#自定义CA #客户端证书认证 http = urllib3.PoolManager( #证书文件路径 cert_file=\u0026#39;/path/to/client_cert.pem\u0026#39;, #证书对应私钥文件路径 key_file=\u0026#39;/path/to/client_key.pem\u0026#39;) 代理配置：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 #HTTP代理 http = urllib3.ProxyManager( #代理服务器地址 \u0026#39;http://proxy.example.com:8080/\u0026#39;, #身份认证信息 proxy_headers={\u0026#39;Proxy-Authorization\u0026#39;: \u0026#39;Basic ...\u0026#39;}) #SOCKS代理(需要安装PySocks) pip install pysocks from urllib3.contrib.socks import SOCKSProxyManager proxy = SOCKSProxyManager( \u0026#39;socks5://user:password@127.0.0.1:1080/\u0026#39; ) 性能优化技巧 连接池调优：\n1 2 3 4 5 6 7 #根据应用场景调整连接池参数 http = urllib3.PoolManager( num_pools=10, # 适合大多数应用 maxsize=10, # 每个连接池最大连接数 block=True, # 连接池满时阻塞而非创建新连接 timeout=60.0 # 适当延长超时时间 ) 连接重用：\n1 2 3 4 #使用上下文管理器确保连接正确释放 with http.request(\u0026#39;GET\u0026#39;, \u0026#39;http://example.com\u0026#39;, preload_content=False) as response: process_response(response) #连接自动返回到连接池 批处理请求（使用线程池并发发起请求，并收集响应对象）：\n1 2 3 4 5 6 7 8 9 10 11 from concurrent.futures import ThreadPoolExecutor urls = [\u0026#39;http://example.com/1\u0026#39;, \u0026#39;http://example.com/2\u0026#39;, \u0026#39;http://example.com/3\u0026#39;] def fetch(url): return http.request(\u0026#39;GET\u0026#39;, url) with ThreadPoolExecutor(max_workers=5) as executor: results = list(executor.map(fetch, urls)) 6.常见的应用场景 Web API调用：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 import json from urllib.parse import urlencode base_url = \u0026#34;https://api.example.com/v1\u0026#34; def get_user(user_id): response = http.request( \u0026#39;GET\u0026#39;, f\u0026#34;{base_url}/users/{user_id}\u0026#34;, headers={\u0026#39;Authorization\u0026#39;: \u0026#39;Bearer token123\u0026#39;} ) return json.loads(response.data.decode(\u0026#39;utf-8\u0026#39;)) def search_users(query, limit=10): params = {\u0026#39;q\u0026#39;: query, \u0026#39;limit\u0026#39;: limit} response = http.request( \u0026#39;GET\u0026#39;, f\u0026#34;{base_url}/users/search?{urlencode(params)}\u0026#34; ) return json.loads(response.data.decode(\u0026#39;utf-8\u0026#39;)) 网页抓取：\n1 2 3 4 5 6 7 8 9 10 11 12 13 from bs4 import BeautifulSoup def scrape_website(url): response = http.request(\u0026#39;GET\u0026#39;, url) if response.status == 200: soup = BeautifulSoup(response.data, \u0026#39;html.parser\u0026#39;) # 提取数据... return { \u0026#39;title\u0026#39;: soup.title.string, #遍历所有的a标签，并提取其中的href属性 \u0026#39;links\u0026#39;: [a[\u0026#39;href\u0026#39;] for a in soup.find_all(\u0026#39;a\u0026#39;)] } return None 文件下载：\n1 2 3 4 5 6 7 8 def download_file(url, save_path): with http.request(\u0026#39;GET\u0026#39;, url, preload_content=False) as response: if response.status == 200: with open(save_path, \u0026#39;wb\u0026#39;) as f: for chunk in response.stream(1024): f.write(chunk) return True return False 微服务通信：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 import json def call_service(service_url, method, payload=None): headers = { \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;, \u0026#39;X-Request-ID\u0026#39;: \u0026#39;unique-id-123\u0026#39; } body = json.dumps(payload).encode(\u0026#39;utf-8\u0026#39;) if payload else None response = http.request( method.upper(), service_url, headers=headers, body=body ) if response.status \u0026gt;= 400: raise Exception(f\u0026#34;Service error: {response.status}\u0026#34;) return json.loads(response.data.decode(\u0026#39;utf-8\u0026#39;)) 常见问题 异常处理：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 import urllib3.exceptions try: response = http.request(\u0026#39;GET\u0026#39;, \u0026#39;http://example.com\u0026#39;) except urllib3.exceptions.HTTPError as e: print(f\u0026#34;HTTP错误: {e}\u0026#34;) except urllib3.exceptions.SSLError as e: print(f\u0026#34;SSL错误: {e}\u0026#34;) except urllib3.exceptions.TimeoutError as e: print(f\u0026#34;请求超时: {e}\u0026#34;) except urllib3.exceptions.RequestError as e: print(f\u0026#34;请求错误: {e}\u0026#34;) except Exception as e: print(f\u0026#34;其他错误: {e}\u0026#34;) 调试技巧：\n1 2 3 4 5 6 7 8 9 10 11 #启用调试日志 import logging logging.basicConfig(level=logging.DEBUG) #或者只启用urllib3的调试日志 logger = logging.getLogger(\u0026#39;urllib3\u0026#39;) logger.setLevel(logging.DEBUG) #查看连接池状态 print(http.connection_pool_kw) print(http.pools) 与request库的对比 更底层的控制：直接访问连接池和底层配置 更小的内存占用：没有 requests 的额外抽象层 更早的错误检测：在请求发送前就能检测到某些问题 更灵活的流处理：对大型文件或流式API更友好 大多数情况用request，精细控制或高性能用urllib3 一些问题 Q 当我向https://www.xiaohongshu.com/发起请求失败时，urllib3会自动重新定向到https://www.xiaohongshu.com/explore，这是为什么，程序为什么会知道https://www.xiaohongshu.com/explore是正确的 A 不是urllib3“知道正确地址”，重新定向是服务端的行为，当向https://www.xiaohongshu.com/发起请求后，小红书的服务器会返回一个HTTP 301或302响应，将正确的网址返回给客户端，同时urllib会默认自动跟随重定向。 Q JSON响应处理和流式响应处理的区别 A 对于JSON响应处理，response.data一次性读取全部内容，再将其解析为Python对象；流式响应处理，不会把整个响应体加载进内存。JSON响应处理适用于REST API、结构化数据；流式响应处理适用于下载大文件、音视频流等场景 Q 线程与并发的关系 A 线程是并发的一种实现方式，但并发并不限于线程，线程像“工人”，并发像“工人排班的调度方式”，一个线程处理一个任务，多个线程合作是实现 Q PoolManager和ProxyManager的区别 A ProxyManager是继承自PoolManager，专门用来处理从代理服务器发出的请求，其重写了PoolManager的部分方法，确保所有的请求都从预定的代理进行，并且它能根据目标的URL的方案和代理类型，智能的选择使用HTTP转发或HTTP CONNECT通道 一些测试代码 一些库导入\n1 2 3 4 5 6 7 import urllib3 import threading import time from urllib3.exceptions import EmptyPoolError import json import logging logging.basicConfig(level=logging.DEBUG) 测试连接复用：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 # 创建连接池，最大连接数设置为 1（便于观察复用） http = urllib3.HTTPConnectionPool(\u0026#34;httpbin.org\u0026#34;, maxsize=1) # 第一次请求 print(\u0026#34;第1次请求\u0026#34;) r1 = http.request(\u0026#34;GET\u0026#34;, \u0026#34;/get\u0026#34;) print(\u0026#34;响应状态码:\u0026#34;, r1.status) # 暂停 1 秒 time.sleep(1) # 第二次请求 print(\u0026#34;\\n第2次请求\u0026#34;) r2 = http.request(\u0026#34;GET\u0026#34;, \u0026#34;/get\u0026#34;) print(\u0026#34;响应状态码:\u0026#34;, r2.status) \u0026#39;\u0026#39;\u0026#39; #通过如下方式，检查底层 socket 是否复用 print(\u0026#34;连接池中连接数量:\u0026#34;, len(http.pool)) for conn in http.pool: print(\u0026#34;连接对象:\u0026#34;, conn) \u0026#39;\u0026#39;\u0026#39; \u0026#39;\u0026#39;\u0026#39; #测试“不复用的情况”，可以设置 retries=0，并且让服务器关闭连接 r = http.request(\u0026#34;GET\u0026#34;, \u0026#34;/get\u0026#34;, headers={\u0026#34;Connection\u0026#34;: \u0026#34;close\u0026#34;}) \u0026#39;\u0026#39;\u0026#39; 测试限制连接数+阻塞模式:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 # 设置连接池为 maxsize=1，block=False http = urllib3.HTTPConnectionPool(\u0026#34;httpbin.org\u0026#34;, maxsize=1, block=False) # 请求函数：保持连接占用一段时间 def make_request(name): print(f\u0026#34;[{name}] 准备发送请求\u0026#34;) try: r = http.request(\u0026#34;GET\u0026#34;, \u0026#34;/delay/5\u0026#34;, preload_content=False, timeout=10.0) print(f\u0026#34;[{name}] 已发送请求，延迟读取内容\u0026#34;) time.sleep(6) # 保持连接被占用 r.release_conn() # 主动释放连接 print(f\u0026#34;[{name}] 释放连接\u0026#34;) except EmptyPoolError as e: print(f\u0026#34;[{name}] 连接池耗尽异常: {e}\u0026#34;) except Exception as e: print(f\u0026#34;[{name}] 其他异常: {e}\u0026#34;) # 第一个线程：占住唯一连接 t1 = threading.Thread(target=make_request, args=(\u0026#34;线程1\u0026#34;,)) # 第二个线程：争抢连接 t2 = threading.Thread(target=make_request, args=(\u0026#34;线程2\u0026#34;,)) t1.start() time.sleep(0.2) # 保证线程1先占住连接 t2.start() t1.join() t2.join() \u0026#39;\u0026#39;\u0026#39; 连接池已满且不允许阻塞新的请求时，新的连接请求将会被丢弃。这意味着连接池无法处理所有的请求，可能会导致一些请求失败或被延迟处理。 所以在触发连接池已满的情况下不会报错且能正常运行，但在DEBUG日志下会有警告。 \u0026#39;\u0026#39;\u0026#39; 验证连接池（num_pool）作用:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 \u0026#39;num_pool控制的是最多允许缓存多少个不同主机的连接池，每一个host:port组合会对应一个connectionpool实例，当请求的host数超过num_pools时，旧的连接池将会被清理(LRU策略)\u0026#39; # 创建连接池管理器，最多2个主机连接池 http = urllib3.PoolManager(num_pools=2, maxsize=1) hosts = [ \u0026#34;http://httpbin.org/get\u0026#34;, \u0026#34;https://www.xiaohongshu.com/explore\u0026#34;, \u0026#34;http://baidu.com\u0026#34; # 任意有效网址也可 ] def fetch(url): try: r = http.request(\u0026#34;GET\u0026#34;, url) print(f\u0026#34;访问 {url} 状态码: {r.status}\u0026#34;) except Exception as e: print(f\u0026#34;访问 {url} 出错: {e}\u0026#34;) # 访问3个不同host，超出num_pools限制 for i in range(2): for url in hosts: fetch(url) time.sleep(0.5) \u0026#39;\u0026#39;\u0026#39; 可以观察到每次发送请求前都得先建立连接，这是因为只能允许存在两个连接池，当继续往里添加连接池时，会把旧的清理，使得再次连接时需要重新建立连接 \u0026#39;\u0026#39;\u0026#39; ","date":"2025-08-05T12:10:50+08:00","image":"https://Entars.github.io/p/urllib3/urllib3_hu_c05e892ee201c66c.png","permalink":"https://Entars.github.io/p/urllib3/","title":"Urllib3"},{"content":"正文测试 而这些并不是完全重要，更加重要的问题是， 带着这些问题，我们来审视一下学生会退会。 既然如何， 对我个人而言，学生会退会不仅仅是一个重大的事件，还可能会改变我的人生。 我们不得不面对一个非常尴尬的事实，那就是， 可是，即使是这样，学生会退会的出现仍然代表了一定的意义。 学生会退会，发生了会如何，不发生又会如何。 经过上述讨论， 生活中，若学生会退会出现了，我们就不得不考虑它出现了的事实。 学生会退会，到底应该如何实现。 这样看来， 在这种困难的抉择下，本人思来想去，寝食难安。 对我个人而言，学生会退会不仅仅是一个重大的事件，还可能会改变我的人生。 就我个人来说，学生会退会对我的意义，不能不说非常重大。 莎士比亚曾经提到过，人的一生是短的，但如果卑劣地过这一生，就太长了。这似乎解答了我的疑惑。 莫扎特说过一句富有哲理的话，谁和我一样用功，谁就会和我一样成功。这启发了我， 对我个人而言，学生会退会不仅仅是一个重大的事件，还可能会改变我的人生。 学生会退会，到底应该如何实现。 一般来说， 从这个角度来看， 这种事实对本人来说意义重大，相信对这个世界也是有一定意义的。 在这种困难的抉择下，本人思来想去，寝食难安。 了解清楚学生会退会到底是一种怎么样的存在，是解决一切问题的关键。 一般来说， 生活中，若学生会退会出现了，我们就不得不考虑它出现了的事实。 问题的关键究竟为何？ 而这些并不是完全重要，更加重要的问题是。\n奥斯特洛夫斯基曾经说过，共同的事业，共同的斗争，可以使人们产生忍受一切的力量。　带着这句话，我们还要更加慎重的审视这个问题： 一般来讲，我们都必须务必慎重的考虑考虑。 既然如此， 这种事实对本人来说意义重大，相信对这个世界也是有一定意义的。 带着这些问题，我们来审视一下学生会退会。 我认为， 我认为， 在这种困难的抉择下，本人思来想去，寝食难安。 问题的关键究竟为何？ 每个人都不得不面对这些问题。 在面对这种问题时， 要想清楚，学生会退会，到底是一种怎么样的存在。 我认为， 既然如此， 每个人都不得不面对这些问题。 在面对这种问题时， 那么， 我认为， 学生会退会因何而发生。\n引用 思念是最暖的忧伤像一双翅膀\n让我停不了飞不远在过往游荡\n不告而别的你 就算为了我着想\n这么沉痛的呵护 我怎么能翱翔\n最暖的憂傷 - 田馥甄\n图片 1 2 3 ![Photo by Florian Klauer on Unsplash](florian-klauer-nptLmg6jqDo-unsplash.jpg) ![Photo by Luca Bravo on Unsplash](luca-bravo-alS7ewQ41M8-unsplash.jpg) ![Photo by Helena Hertz on Unsplash](helena-hertz-wWZzXlDpMog-unsplash.jpg) ![Photo by Hudai Gayiran on Unsplash](hudai-gayiran-3Od_VKcDEAA-unsplash.jpg) 相册语法来自 Typlog\n","date":"2020-09-09T00:00:00Z","image":"https://Entars.github.io/p/test-chinese/helena-hertz-wWZzXlDpMog-unsplash_hu_2307260c751d0e0b.jpg","permalink":"https://Entars.github.io/p/test-chinese/","title":"Chinese Test"},{"content":"This article offers a sample of basic Markdown syntax that can be used in Hugo content files, also it shows whether basic HTML elements are decorated with CSS in a Hugo theme.\nHeadings The following HTML \u0026lt;h1\u0026gt;—\u0026lt;h6\u0026gt; elements represent six levels of section headings. \u0026lt;h1\u0026gt; is the highest section level while \u0026lt;h6\u0026gt; is the lowest.\nH1 H2 H3 H4 H5 H6 Paragraph Xerum, quo qui aut unt expliquam qui dolut labo. Aque venitatiusda cum, voluptionse latur sitiae dolessi aut parist aut dollo enim qui voluptate ma dolestendit peritin re plis aut quas inctum laceat est volestemque commosa as cus endigna tectur, offic to cor sequas etum rerum idem sintibus eiur? Quianimin porecus evelectur, cum que nis nust voloribus ratem aut omnimi, sitatur? Quiatem. Nam, omnis sum am facea corem alique molestrunt et eos evelece arcillit ut aut eos eos nus, sin conecerem erum fuga. Ri oditatquam, ad quibus unda veliamenimin cusam et facea ipsamus es exerum sitate dolores editium rerore eost, temped molorro ratiae volorro te reribus dolorer sperchicium faceata tiustia prat.\nItatur? Quiatae cullecum rem ent aut odis in re eossequodi nonsequ idebis ne sapicia is sinveli squiatum, core et que aut hariosam ex eat.\nBlockquotes The blockquote element represents content that is quoted from another source, optionally with a citation which must be within a footer or cite element, and optionally with in-line changes such as annotations and abbreviations.\nBlockquote without attribution Tiam, ad mint andaepu dandae nostion secatur sequo quae. Note that you can use Markdown syntax within a blockquote.\nBlockquote with attribution Don\u0026rsquo;t communicate by sharing memory, share memory by communicating.\n— Rob Pike1\nTables Tables aren\u0026rsquo;t part of the core Markdown spec, but Hugo supports supports them out-of-the-box.\nName Age Bob 27 Alice 23 Inline Markdown within tables Italics Bold Code italics bold code A B C D E F Lorem ipsum dolor sit amet, consectetur adipiscing elit. Phasellus ultricies, sapien non euismod aliquam, dui ligula tincidunt odio, at accumsan nulla sapien eget ex. Proin eleifend dictum ipsum, non euismod ipsum pulvinar et. Vivamus sollicitudin, quam in pulvinar aliquam, metus elit pretium purus Proin sit amet velit nec enim imperdiet vehicula. Ut bibendum vestibulum quam, eu egestas turpis gravida nec Sed scelerisque nec turpis vel viverra. Vivamus vitae pretium sapien Code Blocks Code block with backticks 1 2 3 4 5 6 7 8 9 10 \u0026lt;!doctype html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;utf-8\u0026#34;\u0026gt; \u0026lt;title\u0026gt;Example HTML5 Document\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p\u0026gt;Test\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Code block indented with four spaces \u0026lt;!doctype html\u0026gt; \u0026lt;html lang=\u0026quot;en\u0026quot;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026quot;utf-8\u0026quot;\u0026gt; \u0026lt;title\u0026gt;Example HTML5 Document\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p\u0026gt;Test\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Code block with Hugo\u0026rsquo;s internal highlight shortcode 1 2 3 4 5 6 7 8 9 10 \u0026lt;!doctype html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;utf-8\u0026#34;\u0026gt; \u0026lt;title\u0026gt;Example HTML5 Document\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p\u0026gt;Test\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Diff code block 1 2 3 4 5 [dependencies.bevy] git = \u0026#34;https://github.com/bevyengine/bevy\u0026#34; rev = \u0026#34;11f52b8c72fc3a568e8bb4a4cd1f3eb025ac2e13\u0026#34; - features = [\u0026#34;dynamic\u0026#34;] + features = [\u0026#34;jpeg\u0026#34;, \u0026#34;dynamic\u0026#34;] List Types Ordered List First item Second item Third item Unordered List List item Another item And another item Nested list Fruit Apple Orange Banana Dairy Milk Cheese Other Elements — abbr, sub, sup, kbd, mark GIF is a bitmap image format.\nH2O\nXn + Yn = Zn\nPress CTRL + ALT + Delete to end the session.\nMost salamanders are nocturnal, and hunt for insects, worms, and other small creatures.\nHyperlinked image The above quote is excerpted from Rob Pike\u0026rsquo;s talk during Gopherfest, November 18, 2015.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"2019-03-11T00:00:00Z","image":"https://Entars.github.io/p/markdown-syntax-guide/pawel-czerwinski-8uZPynIu-rQ-unsplash_hu_e95a4276bf860a84.jpg","permalink":"https://Entars.github.io/p/markdown-syntax-guide/","title":"Markdown Syntax Guide"},{"content":"Lorem est tota propiore conpellat pectoribus de pectora summo.\nRedit teque digerit hominumque toris verebor lumina non cervice subde tollit usus habet Arctonque, furores quas nec ferunt. Quoque montibus nunc caluere tempus inhospita parcite confusaque translucet patri vestro qui optatis lumine cognoscere flos nubis! Fronde ipsamque patulos Dryopen deorum.\nExierant elisi ambit vivere dedere Duce pollice Eris modo Spargitque ferrea quos palude Rursus nulli murmur; hastile inridet ut ab gravi sententia! Nomine potitus silentia flumen, sustinet placuit petis in dilapsa erat sunt. Atria tractus malis.\nComas hunc haec pietate fetum procerum dixit Post torum vates letum Tiresia Flumen querellas Arcanaque montibus omnes Quidem et Vagus elidunt The Van de Graaf Canon\nMane refeci capiebant unda mulcebat Victa caducifer, malo vulnere contra dicere aurato, ludit regale, voca! Retorsit colit est profanae esse virescere furit nec; iaculi matertera et visa est, viribus. Divesque creatis, tecta novat collumque vulnus est, parvas. Faces illo pepulere tempus adest. Tendit flamma, ab opes virum sustinet, sidus sequendo urbis.\nIubar proles corpore raptos vero auctor imperium; sed et huic: manus caeli Lelegas tu lux. Verbis obstitit intus oblectamina fixis linguisque ausus sperare Echionides cornuaque tenent clausit possit. Omnia putatur. Praeteritae refert ausus; ferebant e primus lora nutat, vici quae mea ipse. Et iter nil spectatae vulnus haerentia iuste et exercebat, sui et.\nEurytus Hector, materna ipsumque ut Politen, nec, nate, ignari, vernum cohaesit sequitur. Vel mitis temploque vocatus, inque alis, oculos nomen non silvis corpore coniunx ne displicet illa. Crescunt non unus, vidit visa quantum inmiti flumina mortis facto sic: undique a alios vincula sunt iactata abdita! Suspenderat ego fuit tendit: luna, ante urbem Propoetides parte.\n","date":"2019-03-09T00:00:00Z","image":"https://Entars.github.io/p/placeholder-text/matt-le-SJSpo9hQf7s-unsplash_hu_c1ca39d792aee4ab.jpg","permalink":"https://Entars.github.io/p/placeholder-text/","title":"Placeholder Text"},{"content":"Mathematical notation in a Hugo project can be enabled by using third party JavaScript libraries.\nIn this example we will be using KaTeX\nCreate a partial under /layouts/partials/math.html Within this partial reference the Auto-render Extension or host these scripts locally. Include the partial in your templates like so: 1 2 3 {{ if or .Params.math .Site.Params.math }} {{ partial \u0026#34;math.html\u0026#34; . }} {{ end }} To enable KaTeX globally set the parameter math to true in a project\u0026rsquo;s configuration To enable KaTeX on a per page basis include the parameter math: true in content files Note: Use the online reference of Supported TeX Functions\nExamples Inline math: $\\varphi = \\dfrac{1+\\sqrt5}{2}= 1.6180339887…$\nBlock math: $$ \\varphi = 1+\\frac{1} {1+\\frac{1} {1+\\frac{1} {1+\\cdots} } } $$","date":"2019-03-08T00:00:00Z","permalink":"https://Entars.github.io/p/math-typesetting/","title":"Math Typesetting"},{"content":"Emoji can be enabled in a Hugo project in a number of ways.\nThe emojify function can be called directly in templates or Inline Shortcodes.\nTo enable emoji globally, set enableEmoji to true in your site\u0026rsquo;s configuration and then you can type emoji shorthand codes directly in content files; e.g.\n🙈 :see_no_evil: 🙉 :hear_no_evil: 🙊 :speak_no_evil:\nThe Emoji cheat sheet is a useful reference for emoji shorthand codes.\nN.B. The above steps enable Unicode Standard emoji characters and sequences in Hugo, however the rendering of these glyphs depends on the browser and the platform. To style the emoji you can either use a third party emoji font or a font stack; e.g.\n1 2 3 .emoji { font-family: Apple Color Emoji, Segoe UI Emoji, NotoColorEmoji, Segoe UI Symbol, Android Emoji, EmojiSymbols; } ","date":"2019-03-05T00:00:00Z","image":"https://Entars.github.io/p/emoji-support/the-creative-exchange-d2zvqp3fpro-unsplash_hu_27b8954607cdb515.jpg","permalink":"https://Entars.github.io/p/emoji-support/","title":"Emoji Support"}]